{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-04T05:26:15.612012Z",
     "start_time": "2019-12-04T05:26:12.022110Z"
    }
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import numba\n",
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import nltk\n",
    "from dask.multiprocessing import get\n",
    "from gensim.models import Word2Vec, Phrases\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Bidirectional, Dropout, Dense, Attention, Embedding, Conv1D, MaxPool1D\n",
    "import tqdm\n",
    "from multiprocessing import cpu_count\n",
    "\n",
    "#Configuraciones \n",
    "from tensorflow.compat.v1 import ConfigProto\n",
    "from tensorflow.compat.v1 import InteractiveSession\n",
    "\n",
    "config = ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.2\n",
    "config.gpu_options.allow_growth = True\n",
    "session = InteractiveSession(config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-04T05:26:16.868298Z",
     "start_time": "2019-12-04T05:26:16.861953Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset_1_zip = zipfile.ZipFile('/home/alberto/Escritorio/pycharm/NLP/ignore_files/IMDB Dataset.csv.zip', 'r')\n",
    "dataset_1_csv = dataset_1_zip.open('IMDB Dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-04T05:26:18.034256Z",
     "start_time": "2019-12-04T05:26:17.245970Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  One of the other reviewers has mentioned that ...          1\n",
       "1  A wonderful little production. <br /><br />The...          1\n",
       "2  I thought this was a wonderful way to spend ti...          1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_1_train = pd.read_csv(dataset_1_csv)\n",
    "#positive and negative sentimen to categorical\n",
    "df_1_train['sentiment'] = pd.Categorical(df_1_train.sentiment).codes\n",
    "df_1_train[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-04T05:26:18.047868Z",
     "start_time": "2019-12-04T05:26:18.044035Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset_2_zip = zipfile.ZipFile('/home/alberto/Escritorio/pycharm/NLP/ignore_files/word2vec-nlp-tutorial.zip')\n",
    "dataset_2_csv_train = dataset_2_zip.open('labeledTrainData.tsv')\n",
    "dataset_2_csv_test = dataset_2_zip.open('testData.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-04T05:26:19.006596Z",
     "start_time": "2019-12-04T05:26:18.226638Z"
    }
   },
   "outputs": [],
   "source": [
    "df_2_train = pd.read_csv(dataset_2_csv_train, sep='\\t')\n",
    "df_test = pd.read_csv(dataset_2_csv_test, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-04T05:26:19.033306Z",
     "start_time": "2019-12-04T05:26:19.025073Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              review  sentiment\n",
      "0  One of the other reviewers has mentioned that ...          1\n",
      "1  A wonderful little production. <br /><br />The...          1\n",
      "2  I thought this was a wonderful way to spend ti...          1\n",
      "3  Basically there's a family where a little boy ...          0\n",
      "4  Petter Mattei's \"Love in the Time of Money\" is...          1\n",
      "       id  sentiment                                             review\n",
      "0  5814_8          1  With all this stuff going down at the moment w...\n",
      "1  2381_9          1  \\The Classic War of the Worlds\\\" by Timothy Hi...\n",
      "2  7759_3          0  The film starts with a manager (Nicholas Bell)...\n",
      "3  3630_4          0  It must be assumed that those who praised this...\n",
      "4  9495_8          1  Superbly trashy and wondrously unpretentious 8...\n",
      "         id                                             review\n",
      "0  12311_10  Naturally in a film who's main themes are of m...\n",
      "1    8348_2  This movie is a disaster within a disaster fil...\n",
      "2    5828_4  All in all, this is a movie for kids. We saw i...\n",
      "3    7186_2  Afraid of the Dark left me with the impression...\n",
      "4   12128_7  A very accurate depiction of small time mob li...\n"
     ]
    }
   ],
   "source": [
    "print(df_1_train.head())\n",
    "print(df_2_train.head())\n",
    "print(df_test.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that our first dataset doesn't has the column \"id\" so we will drop this column fromo our datasets and let concat the dataframe train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-04T05:26:22.590194Z",
     "start_time": "2019-12-04T05:26:22.533859Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "columnas train: Index(['review', 'sentiment'], dtype='object')\n",
      "columnas test: Index(['id', 'review'], dtype='object')\n",
      "Shape train: (75000, 2)\n",
      "Shape test: (25000, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  One of the other reviewers has mentioned that ...          1\n",
       "1  A wonderful little production. <br /><br />The...          1\n",
       "2  I thought this was a wonderful way to spend ti...          1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df_test.drop(columns='id', inplace=True)\n",
    "df_train = pd.concat([df_1_train, df_2_train[['sentiment', 'review']]], axis=0, sort=False)\n",
    "df_train.reset_index(drop=True, inplace=True)\n",
    "print('columnas train:', df_train.columns)\n",
    "print('columnas test:', df_test.columns)\n",
    "print('Shape train:', df_train.shape)\n",
    "print('Shape test:', df_test.shape)\n",
    "df_train.review = df_train.review.astype(str)\n",
    "df_train[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-30T10:38:43.898379Z",
     "start_time": "2019-11-30T10:38:43.894932Z"
    }
   },
   "source": [
    "## Pass everything to Dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-01T10:27:50.099128Z",
     "start_time": "2019-12-01T10:27:50.094889Z"
    }
   },
   "outputs": [],
   "source": [
    "#count number of threads\n",
    "cores = cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-01T10:28:26.169749Z",
     "start_time": "2019-12-01T10:28:25.794241Z"
    }
   },
   "outputs": [],
   "source": [
    "#Create 2 empty columns for dask text processing\n",
    "df_train['procesed'] = ''\n",
    "df_test['procesed'] = ''\n",
    "dask_train = dd.from_pandas(df_train, npartitions=cores)\n",
    "dask_test = dd.from_pandas(df_test, npartitions=cores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Pre - Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-01T10:28:27.415702Z",
     "start_time": "2019-12-01T10:28:27.400783Z"
    }
   },
   "outputs": [],
   "source": [
    "#Remove html\n",
    "def parse_html(nlp):\n",
    "    return BeautifulSoup(nlp).get_text()\n",
    "\n",
    "#Remove non words\n",
    "def remove_nonwords(nlp):\n",
    "    return re.sub(\"[^a-zA-Z]\", \" \", nlp)\n",
    "\n",
    "#Lower all text\n",
    "def lower(nlp):\n",
    "    return nlp.lower()\n",
    "\n",
    "#remove stopwords\n",
    "def remove_stopwords(nlp):\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "    splited = nlp.split()\n",
    "    removed = [item for item in splited if item not in stopwords]\n",
    "    joined = ' '.join(removed)\n",
    "    return joined\n",
    "\n",
    "#tokenize text\n",
    "def own_tokenizer(nlp):\n",
    "    return nltk.word_tokenize(nlp)\n",
    "\n",
    "#lematize words\n",
    "def own_lemmatizer(nlp):\n",
    "    lemmatizer = nltk.stem.WordNetLemmatizer()            \n",
    "    lemmas = list(map(lemmatizer.lemmatize, nlp))\n",
    "    return lemmas\n",
    "\n",
    "#a fuction that group all the preprocess functions\n",
    "def df_clean(df_train):\n",
    "    df_train['procesed'] = df_train.review.map(parse_html).map(remove_nonwords).map(lower).map(remove_stopwords).map(\n",
    "    own_tokenizer).map(own_lemmatizer)\n",
    "    return df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-01T10:29:11.159468Z",
     "start_time": "2019-12-01T10:28:31.558148Z"
    }
   },
   "outputs": [],
   "source": [
    "dask_train_processed = dask_train.map_partitions(df_clean, meta=df_train)\n",
    "dask_test_processed = dask_test.map_partitions(df_clean, meta=df_test)\n",
    "preprocessed_train = dask_train_processed.compute(scheduler='processes')\n",
    "preprocessed_test = dask_test_processed.compute(scheduler='processes')\n",
    "preprocessed_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-16T08:56:15.970836Z",
     "start_time": "2019-10-16T08:56:15.961833Z"
    }
   },
   "source": [
    "# mini-EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-01T10:29:38.675957Z",
     "start_time": "2019-12-01T10:29:38.670394Z"
    }
   },
   "outputs": [],
   "source": [
    "print('Balance of the classes in train:',  preprocessed_train.sentiment.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-01T10:31:19.128170Z",
     "start_time": "2019-12-01T10:31:19.084320Z"
    }
   },
   "outputs": [],
   "source": [
    "lenth = preprocessed_train.procesed.map(len)\n",
    "print('Mean', lenth.mean())\n",
    "print('Median', lenth.median())\n",
    "print('Mode', lenth.mode()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing to model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-01T10:40:38.546715Z",
     "start_time": "2019-12-01T10:40:21.031117Z"
    }
   },
   "outputs": [],
   "source": [
    "#Create the vocabulary in Bigrams\n",
    "bigrams = Phrases(sentences=preprocessed_train['procesed'], min_count=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-01T10:41:29.975013Z",
     "start_time": "2019-12-01T10:40:38.866814Z"
    }
   },
   "outputs": [],
   "source": [
    "#Create the vocabulary in Trigrams\n",
    "trigrams = Phrases(sentences=bigrams[preprocessed_train['procesed']], min_count=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-01T10:42:19.928953Z",
     "start_time": "2019-12-01T10:41:30.455878Z"
    }
   },
   "outputs": [],
   "source": [
    "#Create the vocabulary in cuatrigrams\n",
    "fourgrams = Phrases(sentences=trigrams[preprocessed_train['procesed']], min_count=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-01T10:48:09.024788Z",
     "start_time": "2019-12-01T10:43:08.407403Z"
    }
   },
   "outputs": [],
   "source": [
    "#Bigram model\n",
    "#window is the maximun distance between the current and predicted word within a sentence\n",
    "embedding_size = 256\n",
    "bigram_model = Word2Vec(sentences = bigrams[preprocessed_train['procesed']], size=embedding_size,\n",
    "                        min_count=4, window=5, workers=cores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-01T10:56:54.852152Z",
     "start_time": "2019-12-01T10:48:09.362970Z"
    }
   },
   "outputs": [],
   "source": [
    "#Trigram model\n",
    "trigrams_model = Word2Vec(sentences = trigrams[bigrams[preprocessed_train['procesed']]], size=embedding_size,\n",
    "                        min_count=3, window=5, workers=cores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-01T11:05:53.969338Z",
     "start_time": "2019-12-01T10:57:20.091938Z"
    }
   },
   "outputs": [],
   "source": [
    "#Fourgram model\n",
    "fourgram_model = Word2Vec(sentences = fourgrams[trigrams[preprocessed_train['procesed']]], size=embedding_size,\n",
    "                        min_count=3, window=5, workers=cores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-01T10:56:55.283415Z",
     "start_time": "2019-12-01T10:56:33.946Z"
    }
   },
   "source": [
    "### Test the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-01T11:07:57.969052Z",
     "start_time": "2019-12-01T11:07:57.783878Z"
    }
   },
   "outputs": [],
   "source": [
    "bigram_model.wv.most_similar('america')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-01T11:07:59.101689Z",
     "start_time": "2019-12-01T11:07:58.864109Z"
    }
   },
   "outputs": [],
   "source": [
    "trigrams_model.wv.most_similar('america')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-01T11:08:00.038992Z",
     "start_time": "2019-12-01T11:07:59.790160Z"
    }
   },
   "outputs": [],
   "source": [
    "fourgram_model.wv.most_similar('america')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-01T11:08:31.105824Z",
     "start_time": "2019-12-01T11:08:31.093719Z"
    }
   },
   "outputs": [],
   "source": [
    "X_data = fourgrams[trigrams[bigrams[preprocessed_train['procesed']]]]\n",
    "\n",
    "X_data_test = fourgrams[trigrams[bigrams[preprocessed_test['procesed']]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-01T11:08:33.899039Z",
     "start_time": "2019-12-01T11:08:33.891225Z"
    }
   },
   "outputs": [],
   "source": [
    "def vectorize_data(data, vocab)-> list:\n",
    "    keys = list(vocab.keys())\n",
    "    filter_unknown = lambda word: vocab.get(word, None) is not None\n",
    "    encode = lambda review: list(map(keys.index, filter(filter_unknown, review)))\n",
    "    vectorized = list(map(encode, data))\n",
    "    return vectorized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-01T12:04:26.061457Z",
     "start_time": "2019-12-01T11:08:39.271314Z"
    }
   },
   "outputs": [],
   "source": [
    "word_vector = vectorize_data(X_data, fourgram_model.wv.vocab)\n",
    "word_vector_test = vectorize_data(X_data_test, fourgram_model.wv.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-01T12:04:30.753823Z",
     "start_time": "2019-12-01T12:04:29.695121Z"
    }
   },
   "outputs": [],
   "source": [
    "input_length = 150\n",
    "X_pad = pad_sequences(sequences=word_vector, maxlen=input_length, padding='post')\n",
    "X_pad_test = pad_sequences(sequences=word_vector_test, maxlen=input_length, padding='post')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Guardamos los modelos y variables necesarias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-27T16:41:56.762348Z",
     "start_time": "2019-10-27T16:41:56.746792Z"
    }
   },
   "outputs": [],
   "source": [
    "#Save the models and text transformations\n",
    "fourgram_model.save('/home/alberto/Escritorio/pycharm/NLP/ignore_files/w2v_fourgram.model')\n",
    "pd.DataFrame(X_pad).to_csv('/home/alberto/Escritorio/pycharm/NLP/ignore_files/w2v_padaseq.csv')\n",
    "pd.DataFrame(X_pad_test).to_csv('/home/alberto/Escritorio/pycharm/NLP/ignore_files/w2v_padaseq_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-04T05:27:51.865240Z",
     "start_time": "2019-12-04T05:27:49.962589Z"
    }
   },
   "outputs": [],
   "source": [
    "#Load model and vectors\n",
    "input_length = 150\n",
    "fourgram_model = Word2Vec.load('/home/alberto/Escritorio/pycharm/NLP/ignore_files/w2v_fourgram.model')\n",
    "X_pad = pd.read_csv('/home/alberto/Escritorio/pycharm/NLP/ignore_files/w2v_padaseq.csv')\n",
    "X_pad.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "\n",
    "X_pad_test = pd.read_csv('/home/alberto/Escritorio/pycharm/NLP/ignore_files/w2v_padaseq_test.csv')\n",
    "X_pad_test.drop(columns=['Unnamed: 0'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre_trained glove2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-04T05:28:07.290788Z",
     "start_time": "2019-12-04T05:27:56.945733Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('/home/alberto/Escritorio/pycharm/NLP/ignore_files/glove.840B.300d.txt') as f:\n",
    "    content = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-04T05:32:01.663807Z",
     "start_time": "2019-12-04T05:28:17.798248Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/pycharm/lib/python3.7/site-packages/ipykernel_launcher.py:3: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0106b96fdd544196a5c3045831c83683",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2196017), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "\n",
    "for line in tqdm.tqdm_notebook(content):\n",
    "    values = line.split(' ')\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-04T05:37:24.942200Z",
     "start_time": "2019-12-04T05:37:24.937864Z"
    }
   },
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = 150\n",
    "EMBEDDING_DIM = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-04T05:37:28.574615Z",
     "start_time": "2019-12-04T05:37:28.311202Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3922d142bf9643e2be3cad4f94604845",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#aquí lo que hacemos es quitar las palabras que no están en nuestro dataset\n",
    "embedding_matrix = np.zeros((len(fourgram_model.wv.vocab) + 1, EMBEDDING_DIM))\n",
    "suma = 0\n",
    "for word, i in tqdm.notebook.tqdm(zip(fourgram_model.wv.vocab.keys(), range(len(fourgram_model.wv.vocab.keys())))):\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i-suma] = embedding_vector\n",
    "    else:\n",
    "        suma += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-04T06:34:57.969047Z",
     "start_time": "2019-12-04T06:34:57.164258Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 150, 256)          32472064  \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 150, 64)           49216     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 37, 64)            0         \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 256)               197632    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                16448     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 32,735,425\n",
      "Trainable params: 263,361\n",
      "Non-trainable params: 32,472,064\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(input_dim = fourgram_model.wv.vectors.shape[0],\n",
    "                   output_dim = fourgram_model.wv.vectors.shape[1],\n",
    "                   input_length = input_length,\n",
    "                   weights = [fourgram_model.wv.vectors],\n",
    "                   trainable = False))\n",
    "# model.add(Dropout(0.2))\n",
    "model.add(Conv1D(64, 3, activation='relu', padding='same'))\n",
    "model.add(MaxPool1D(pool_size=4))\n",
    "model.add(Bidirectional(LSTM(128, recurrent_dropout=0.1)))\n",
    "model.add(Dropout(0.20))\n",
    "model.add(Dense(64))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-04T07:11:01.836189Z",
     "start_time": "2019-12-04T06:35:11.620524Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 75000 samples\n",
      "Epoch 1/25\n",
      "75000/75000 [==============================] - 89s 1ms/sample - loss: 0.5375 - accuracy: 0.7205\n",
      "Epoch 2/25\n",
      "75000/75000 [==============================] - 86s 1ms/sample - loss: 0.4320 - accuracy: 0.8001\n",
      "Epoch 3/25\n",
      "75000/75000 [==============================] - 87s 1ms/sample - loss: 0.3895 - accuracy: 0.8245\n",
      "Epoch 4/25\n",
      "75000/75000 [==============================] - 87s 1ms/sample - loss: 0.3565 - accuracy: 0.8429\n",
      "Epoch 5/25\n",
      "75000/75000 [==============================] - 86s 1ms/sample - loss: 0.3274 - accuracy: 0.8588\n",
      "Epoch 6/25\n",
      "75000/75000 [==============================] - 86s 1ms/sample - loss: 0.2926 - accuracy: 0.8757\n",
      "Epoch 7/25\n",
      "75000/75000 [==============================] - 86s 1ms/sample - loss: 0.2577 - accuracy: 0.8925\n",
      "Epoch 8/25\n",
      "75000/75000 [==============================] - 87s 1ms/sample - loss: 0.2203 - accuracy: 0.9109\n",
      "Epoch 9/25\n",
      "75000/75000 [==============================] - 87s 1ms/sample - loss: 0.1810 - accuracy: 0.9281\n",
      "Epoch 10/25\n",
      "75000/75000 [==============================] - 87s 1ms/sample - loss: 0.1505 - accuracy: 0.9413\n",
      "Epoch 11/25\n",
      "75000/75000 [==============================] - 87s 1ms/sample - loss: 0.1292 - accuracy: 0.9500\n",
      "Epoch 12/25\n",
      "75000/75000 [==============================] - 87s 1ms/sample - loss: 0.1029 - accuracy: 0.9614\n",
      "Epoch 13/25\n",
      "75000/75000 [==============================] - 87s 1ms/sample - loss: 0.0912 - accuracy: 0.9651\n",
      "Epoch 14/25\n",
      "75000/75000 [==============================] - 87s 1ms/sample - loss: 0.0829 - accuracy: 0.9687\n",
      "Epoch 15/25\n",
      "75000/75000 [==============================] - 86s 1ms/sample - loss: 0.0750 - accuracy: 0.9725\n",
      "Epoch 16/25\n",
      "75000/75000 [==============================] - 85s 1ms/sample - loss: 0.0626 - accuracy: 0.9769\n",
      "Epoch 17/25\n",
      "75000/75000 [==============================] - 85s 1ms/sample - loss: 0.0632 - accuracy: 0.9766\n",
      "Epoch 18/25\n",
      "75000/75000 [==============================] - 85s 1ms/sample - loss: 0.0580 - accuracy: 0.9781\n",
      "Epoch 19/25\n",
      "75000/75000 [==============================] - 85s 1ms/sample - loss: 0.0519 - accuracy: 0.9805\n",
      "Epoch 20/25\n",
      "75000/75000 [==============================] - 85s 1ms/sample - loss: 0.0494 - accuracy: 0.9818\n",
      "Epoch 21/25\n",
      "75000/75000 [==============================] - 85s 1ms/sample - loss: 0.0491 - accuracy: 0.9819\n",
      "Epoch 22/25\n",
      "75000/75000 [==============================] - 85s 1ms/sample - loss: 0.0464 - accuracy: 0.9832\n",
      "Epoch 23/25\n",
      "75000/75000 [==============================] - 85s 1ms/sample - loss: 0.0453 - accuracy: 0.9838\n",
      "Epoch 24/25\n",
      "75000/75000 [==============================] - 85s 1ms/sample - loss: 0.0445 - accuracy: 0.9835\n",
      "Epoch 25/25\n",
      "75000/75000 [==============================] - 85s 1ms/sample - loss: 0.0410 - accuracy: 0.9849\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f5b8037e850>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=X_pad.values, y=df_train.sentiment.values, batch_size=100, epochs=25)\n",
    "#Write X_pad.values if it's trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-01T17:20:47.311478Z",
     "start_time": "2019-12-01T17:20:46.838986Z"
    }
   },
   "outputs": [],
   "source": [
    "model.save('/home/alberto/Escritorio/pycharm/NLP/ignore_files/sentyment_model_w2v_trained.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('/home/alberto/Escritorio/pycharm/NLP/ignore_files/sentyment_model_w2v_trained.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-01T18:07:47.170335Z",
     "start_time": "2019-12-01T18:06:45.121236Z"
    }
   },
   "outputs": [],
   "source": [
    "#Submit the results to kaggle\n",
    "X_submit = model.predict_classes(X_pad_test.values)\n",
    "submission = pd.merge(df_test.id, pd.DataFrame(X_submit, columns=['sentiment']), left_index=True, right_index=True)\n",
    "submission.to_csv('submission_w2v', index=False)\n",
    "#Kaggle give us a 99.24% of accuracy wich is great"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Glove2vec Pre trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-04T05:37:40.830965Z",
     "start_time": "2019-12-04T05:37:38.101384Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 150, 300)          38053500  \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, 150, 64)           57664     \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 37, 64)            0         \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 256)               197632    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 64)                16448     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 38,325,309\n",
      "Trainable params: 271,809\n",
      "Non-trainable params: 38,053,500\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#esto hay que cambiar la capa de embedding por la de glove\n",
    "model_glove = Sequential()\n",
    "model_glove.add(Embedding(input_dim = len(fourgram_model.wv.vectors) + 1,\n",
    "                   output_dim = EMBEDDING_DIM,\n",
    "                   input_length = input_length,\n",
    "                   weights = [embedding_matrix],\n",
    "                   trainable = False))\n",
    "# model_glove.add(Dropout(0.2))\n",
    "model_glove.add(Conv1D(64, 3, activation='relu', padding='same'))\n",
    "model_glove.add(MaxPool1D(pool_size=4))\n",
    "model_glove.add(Bidirectional(LSTM(128, recurrent_dropout=0.1)))\n",
    "model_glove.add(Dropout(0.25))\n",
    "model_glove.add(Dense(64))\n",
    "model_glove.add(Dropout(0.3))\n",
    "model_glove.add(Dense(1, activation='sigmoid'))\n",
    "model_glove.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model_glove.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-04T06:07:01.175478Z",
     "start_time": "2019-12-04T05:37:44.889131Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 75000 samples\n",
      "Epoch 1/20\n",
      "75000/75000 [==============================] - 90s 1ms/sample - loss: 0.4938 - accuracy: 0.7541\n",
      "Epoch 2/20\n",
      "75000/75000 [==============================] - 85s 1ms/sample - loss: 0.3248 - accuracy: 0.8654\n",
      "Epoch 3/20\n",
      "75000/75000 [==============================] - 85s 1ms/sample - loss: 0.2412 - accuracy: 0.9045\n",
      "Epoch 4/20\n",
      "75000/75000 [==============================] - 86s 1ms/sample - loss: 0.1718 - accuracy: 0.9365\n",
      "Epoch 5/20\n",
      "75000/75000 [==============================] - 86s 1ms/sample - loss: 0.1316 - accuracy: 0.9516\n",
      "Epoch 6/20\n",
      "75000/75000 [==============================] - 86s 1ms/sample - loss: 0.1066 - accuracy: 0.9601\n",
      "Epoch 7/20\n",
      "75000/75000 [==============================] - 86s 1ms/sample - loss: 0.0801 - accuracy: 0.9705\n",
      "Epoch 8/20\n",
      "75000/75000 [==============================] - 89s 1ms/sample - loss: 0.0665 - accuracy: 0.9757\n",
      "Epoch 9/20\n",
      "75000/75000 [==============================] - 91s 1ms/sample - loss: 0.0586 - accuracy: 0.9790\n",
      "Epoch 10/20\n",
      "75000/75000 [==============================] - 93s 1ms/sample - loss: 0.0526 - accuracy: 0.9809\n",
      "Epoch 11/20\n",
      "75000/75000 [==============================] - 91s 1ms/sample - loss: 0.0428 - accuracy: 0.9845\n",
      "Epoch 12/20\n",
      "75000/75000 [==============================] - 91s 1ms/sample - loss: 0.0445 - accuracy: 0.9839\n",
      "Epoch 13/20\n",
      "75000/75000 [==============================] - 89s 1ms/sample - loss: 0.0386 - accuracy: 0.9863\n",
      "Epoch 14/20\n",
      "75000/75000 [==============================] - 87s 1ms/sample - loss: 0.0324 - accuracy: 0.9882\n",
      "Epoch 15/20\n",
      "75000/75000 [==============================] - 88s 1ms/sample - loss: 0.0361 - accuracy: 0.9871\n",
      "Epoch 16/20\n",
      "75000/75000 [==============================] - 88s 1ms/sample - loss: 0.0283 - accuracy: 0.9897\n",
      "Epoch 17/20\n",
      "75000/75000 [==============================] - 87s 1ms/sample - loss: 0.0321 - accuracy: 0.9885\n",
      "Epoch 18/20\n",
      "75000/75000 [==============================] - 87s 1ms/sample - loss: 0.0252 - accuracy: 0.9912\n",
      "Epoch 19/20\n",
      "75000/75000 [==============================] - 87s 1ms/sample - loss: 0.0271 - accuracy: 0.9908\n",
      "Epoch 20/20\n",
      "75000/75000 [==============================] - 87s 1ms/sample - loss: 0.0247 - accuracy: 0.9913\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f5c505d9790>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_glove.fit(x=X_pad.values, y=df_train.sentiment.values, batch_size=100, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-04T06:19:55.163894Z",
     "start_time": "2019-12-04T06:19:54.630978Z"
    }
   },
   "outputs": [],
   "source": [
    "model_glove.save('/home/alberto/Escritorio/pycharm/NLP/ignore_files/sentyment_model_G2V_trained.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-04T06:19:58.820007Z",
     "start_time": "2019-12-04T06:19:58.723322Z"
    }
   },
   "outputs": [],
   "source": [
    "model_glove.load_weights('/home/alberto/Escritorio/pycharm/NLP/ignore_files/sentyment_model_G2V_trained.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-04T06:21:04.952484Z",
     "start_time": "2019-12-04T06:20:01.884262Z"
    }
   },
   "outputs": [],
   "source": [
    "#Submit the results to kaggle\n",
    "X_submit = model_glove.predict_classes(X_pad_test.values)\n",
    "submission = pd.merge(df_test.id, pd.DataFrame(X_submit, columns=['sentiment']), left_index=True, right_index=True)\n",
    "submission.to_csv('submission_G2V', index=False)\n",
    "#Kaggle give us a 98% of accuracy wich is great"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "214.162px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "460.057px",
    "left": "1758.93px",
    "right": "20px",
    "top": "124.952px",
    "width": "471.293px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

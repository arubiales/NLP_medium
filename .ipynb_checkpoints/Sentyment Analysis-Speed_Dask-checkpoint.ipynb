{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-14T20:36:55.418287Z",
     "start_time": "2019-12-14T20:36:55.410615Z"
    }
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import numba\n",
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import nltk\n",
    "from dask.multiprocessing import get\n",
    "from gensim.models import Word2Vec, Phrases\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Bidirectional, Dropout, Dense, Attention, Embedding, Conv1D, MaxPool1D\n",
    "import tqdm\n",
    "from multiprocessing import cpu_count\n",
    "import tensorflow as tf\n",
    "\n",
    "#Configuraciones \n",
    "from tensorflow.compat.v1 import ConfigProto\n",
    "from tensorflow.compat.v1 import InteractiveSession\n",
    "\n",
    "config = ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = InteractiveSession(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-14T19:27:14.358538Z",
     "start_time": "2019-12-14T19:27:14.353691Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset_1_zip = zipfile.ZipFile('/home/rubiales/PycharmProjects/pycharm/NLP/ignore_files/IMDB Dataset.csv.zip', 'r')\n",
    "dataset_1_csv = dataset_1_zip.open('IMDB Dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-14T19:27:16.012360Z",
     "start_time": "2019-12-14T19:27:15.360531Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  One of the other reviewers has mentioned that ...          1\n",
       "1  A wonderful little production. <br /><br />The...          1\n",
       "2  I thought this was a wonderful way to spend ti...          1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_1_train = pd.read_csv(dataset_1_csv)\n",
    "#positive and negative sentimen to categorical\n",
    "df_1_train['sentiment'] = pd.Categorical(df_1_train.sentiment).codes\n",
    "df_1_train[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-14T19:27:16.856622Z",
     "start_time": "2019-12-14T19:27:16.848117Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset_2_zip = zipfile.ZipFile('/home/rubiales/PycharmProjects/pycharm/NLP/ignore_files/word2vec-nlp-tutorial.zip')\n",
    "dataset_2_csv_train = dataset_2_zip.open('labeledTrainData.tsv')\n",
    "dataset_2_csv_test = dataset_2_zip.open('testData.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-14T19:27:18.530835Z",
     "start_time": "2019-12-14T19:27:17.848390Z"
    }
   },
   "outputs": [],
   "source": [
    "df_2_train = pd.read_csv(dataset_2_csv_train, sep='\\t')\n",
    "df_test = pd.read_csv(dataset_2_csv_test, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-14T19:27:18.692240Z",
     "start_time": "2019-12-14T19:27:18.681195Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              review  sentiment\n",
      "0  One of the other reviewers has mentioned that ...          1\n",
      "1  A wonderful little production. <br /><br />The...          1\n",
      "2  I thought this was a wonderful way to spend ti...          1\n",
      "3  Basically there's a family where a little boy ...          0\n",
      "4  Petter Mattei's \"Love in the Time of Money\" is...          1\n",
      "       id  sentiment                                             review\n",
      "0  5814_8          1  With all this stuff going down at the moment w...\n",
      "1  2381_9          1  \\The Classic War of the Worlds\\\" by Timothy Hi...\n",
      "2  7759_3          0  The film starts with a manager (Nicholas Bell)...\n",
      "3  3630_4          0  It must be assumed that those who praised this...\n",
      "4  9495_8          1  Superbly trashy and wondrously unpretentious 8...\n",
      "         id                                             review\n",
      "0  12311_10  Naturally in a film who's main themes are of m...\n",
      "1    8348_2  This movie is a disaster within a disaster fil...\n",
      "2    5828_4  All in all, this is a movie for kids. We saw i...\n",
      "3    7186_2  Afraid of the Dark left me with the impression...\n",
      "4   12128_7  A very accurate depiction of small time mob li...\n"
     ]
    }
   ],
   "source": [
    "print(df_1_train.head())\n",
    "print(df_2_train.head())\n",
    "print(df_test.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that our first dataset doesn't has the column \"id\" so we will drop this column fromo our datasets and let concat the dataframe train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-14T19:27:27.758935Z",
     "start_time": "2019-12-14T19:27:27.681515Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "columnas train: Index(['review', 'sentiment'], dtype='object')\n",
      "columnas test: Index(['id', 'review'], dtype='object')\n",
      "Shape train: (75000, 2)\n",
      "Shape test: (25000, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  One of the other reviewers has mentioned that ...          1\n",
       "1  A wonderful little production. <br /><br />The...          1\n",
       "2  I thought this was a wonderful way to spend ti...          1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df_test.drop(columns='id', inplace=True)\n",
    "df_train = pd.concat([df_1_train, df_2_train[['sentiment', 'review']]], axis=0, sort=False)\n",
    "df_train.reset_index(drop=True, inplace=True)\n",
    "print('columnas train:', df_train.columns)\n",
    "print('columnas test:', df_test.columns)\n",
    "print('Shape train:', df_train.shape)\n",
    "print('Shape test:', df_test.shape)\n",
    "df_train.review = df_train.review.astype(str)\n",
    "df_train[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-30T10:38:43.898379Z",
     "start_time": "2019-11-30T10:38:43.894932Z"
    }
   },
   "source": [
    "## Pass everything to Dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-14T19:27:28.579266Z",
     "start_time": "2019-12-14T19:27:28.576601Z"
    }
   },
   "outputs": [],
   "source": [
    "#count number of threads\n",
    "cores = cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-14T19:27:29.467682Z",
     "start_time": "2019-12-14T19:27:29.169192Z"
    }
   },
   "outputs": [],
   "source": [
    "#Create 2 empty columns for dask text processing\n",
    "df_train['procesed'] = ''\n",
    "df_test['procesed'] = ''\n",
    "dask_train = dd.from_pandas(df_train, npartitions=cores)\n",
    "dask_test = dd.from_pandas(df_test, npartitions=cores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Pre - Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-14T19:27:30.788211Z",
     "start_time": "2019-12-14T19:27:30.777174Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#Remove html\n",
    "def parse_html(nlp):\n",
    "    return BeautifulSoup(nlp).get_text()\n",
    "\n",
    "#Remove non words\n",
    "def remove_nonwords(nlp):\n",
    "    return re.sub(\"[^a-zA-Z]\", \" \", nlp)\n",
    "\n",
    "#Lower all text\n",
    "def lower(nlp):\n",
    "    return nlp.lower()\n",
    "\n",
    "#remove stopwords\n",
    "def remove_stopwords(nlp):\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "    splited = nlp.split()\n",
    "    removed = [item for item in splited if item not in stopwords]\n",
    "    joined = ' '.join(removed)\n",
    "    return joined\n",
    "\n",
    "#tokenize text\n",
    "def own_tokenizer(nlp):\n",
    "    return nltk.word_tokenize(nlp)\n",
    "\n",
    "#lematize words\n",
    "def own_lemmatizer(nlp):\n",
    "    lemmatizer = nltk.stem.WordNetLemmatizer()            \n",
    "    lemmas = list(map(lemmatizer.lemmatize, nlp))\n",
    "    return lemmas\n",
    "\n",
    "#a fuction that group all the preprocess functions\n",
    "def df_clean(df_train):\n",
    "    df_train['procesed'] = df_train.review.map(parse_html).map(remove_nonwords).map(lower).map(remove_stopwords).map(\n",
    "    own_tokenizer).map(own_lemmatizer)\n",
    "    return df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-14T19:27:43.167783Z",
     "start_time": "2019-12-14T19:27:31.258453Z"
    }
   },
   "outputs": [],
   "source": [
    "dask_train_processed = dask_train.map_partitions(df_clean, meta=df_train)\n",
    "dask_test_processed = dask_test.map_partitions(df_clean, meta=df_test)\n",
    "preprocessed_train = dask_train_processed.compute(scheduler='processes')\n",
    "preprocessed_test = dask_test_processed.compute(scheduler='processes')\n",
    "preprocessed_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-16T08:56:15.970836Z",
     "start_time": "2019-10-16T08:56:15.961833Z"
    }
   },
   "source": [
    "# mini-EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-14T19:27:43.170978Z",
     "start_time": "2019-12-14T19:27:32.223Z"
    }
   },
   "outputs": [],
   "source": [
    "print('Balance of the classes in train:',  preprocessed_train.sentiment.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-14T19:27:43.172604Z",
     "start_time": "2019-12-14T19:27:32.831Z"
    }
   },
   "outputs": [],
   "source": [
    "lenth = preprocessed_train.procesed.map(len)\n",
    "print('Mean', lenth.mean())\n",
    "print('Median', lenth.median())\n",
    "print('Mode', lenth.mode()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing to model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-14T19:27:43.174708Z",
     "start_time": "2019-12-14T19:27:36.871Z"
    }
   },
   "outputs": [],
   "source": [
    "#Create the vocabulary in Bigrams\n",
    "bigrams = Phrases(sentences=preprocessed_train['procesed'], min_count=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-14T19:27:43.177754Z",
     "start_time": "2019-12-14T19:27:37.023Z"
    }
   },
   "outputs": [],
   "source": [
    "#Create the vocabulary in Trigrams\n",
    "trigrams = Phrases(sentences=bigrams[preprocessed_train['procesed']], min_count=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-14T19:27:43.180698Z",
     "start_time": "2019-12-14T19:27:37.368Z"
    }
   },
   "outputs": [],
   "source": [
    "#Create the vocabulary in cuatrigrams\n",
    "fourgrams = Phrases(sentences=trigrams[preprocessed_train['procesed']], min_count=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-01T10:48:09.024788Z",
     "start_time": "2019-12-01T10:43:08.407403Z"
    }
   },
   "outputs": [],
   "source": [
    "#Bigram model\n",
    "#window is the maximun distance between the current and predicted word within a sentence\n",
    "embedding_size = 256\n",
    "bigram_model = Word2Vec(sentences = bigrams[preprocessed_train['procesed']], size=embedding_size,\n",
    "                        min_count=4, window=5, workers=cores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-01T10:56:54.852152Z",
     "start_time": "2019-12-01T10:48:09.362970Z"
    }
   },
   "outputs": [],
   "source": [
    "#Trigram model\n",
    "trigrams_model = Word2Vec(sentences = trigrams[bigrams[preprocessed_train['procesed']]], size=embedding_size,\n",
    "                        min_count=3, window=5, workers=cores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-01T11:05:53.969338Z",
     "start_time": "2019-12-01T10:57:20.091938Z"
    }
   },
   "outputs": [],
   "source": [
    "#Fourgram model\n",
    "fourgram_model = Word2Vec(sentences = fourgrams[trigrams[preprocessed_train['procesed']]], size=embedding_size,\n",
    "                        min_count=3, window=5, workers=cores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-01T10:56:55.283415Z",
     "start_time": "2019-12-01T10:56:33.946Z"
    }
   },
   "source": [
    "### Test the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-01T11:07:57.969052Z",
     "start_time": "2019-12-01T11:07:57.783878Z"
    }
   },
   "outputs": [],
   "source": [
    "bigram_model.wv.most_similar('america')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-01T11:07:59.101689Z",
     "start_time": "2019-12-01T11:07:58.864109Z"
    }
   },
   "outputs": [],
   "source": [
    "trigrams_model.wv.most_similar('america')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-01T11:08:00.038992Z",
     "start_time": "2019-12-01T11:07:59.790160Z"
    }
   },
   "outputs": [],
   "source": [
    "fourgram_model.wv.most_similar('america')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-01T11:08:31.105824Z",
     "start_time": "2019-12-01T11:08:31.093719Z"
    }
   },
   "outputs": [],
   "source": [
    "X_data = fourgrams[trigrams[bigrams[preprocessed_train['procesed']]]]\n",
    "\n",
    "X_data_test = fourgrams[trigrams[bigrams[preprocessed_test['procesed']]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-01T11:08:33.899039Z",
     "start_time": "2019-12-01T11:08:33.891225Z"
    }
   },
   "outputs": [],
   "source": [
    "def vectorize_data(data, vocab)-> list:\n",
    "    keys = list(vocab.keys())\n",
    "    filter_unknown = lambda word: vocab.get(word, None) is not None\n",
    "    encode = lambda review: list(map(keys.index, filter(filter_unknown, review)))\n",
    "    vectorized = list(map(encode, data))\n",
    "    return vectorized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-01T12:04:26.061457Z",
     "start_time": "2019-12-01T11:08:39.271314Z"
    }
   },
   "outputs": [],
   "source": [
    "word_vector = vectorize_data(X_data, fourgram_model.wv.vocab)\n",
    "word_vector_test = vectorize_data(X_data_test, fourgram_model.wv.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-08T13:15:15.204577Z",
     "start_time": "2019-12-08T13:15:15.127844Z"
    }
   },
   "outputs": [],
   "source": [
    "input_length = 150\n",
    "X_pad = pad_sequences(sequences=word_vector, maxlen=input_length, padding='post')\n",
    "X_pad_test = pad_sequences(sequences=word_vector_test, maxlen=input_length, padding='post')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Guardamos los modelos y variables necesarias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-27T16:41:56.762348Z",
     "start_time": "2019-10-27T16:41:56.746792Z"
    }
   },
   "outputs": [],
   "source": [
    "#Save the models and text transformations\n",
    "fourgram_model.save('/home/alberto/Escritorio/pycharm/NLP/ignore_files/w2v_fourgram.model')\n",
    "pd.DataFrame(X_pad).to_csv('/home/alberto/Escritorio/pycharm/NLP/ignore_files/w2v_padaseq.csv')\n",
    "pd.DataFrame(X_pad_test).to_csv('/home/alberto/Escritorio/pycharm/NLP/ignore_files/w2v_padaseq_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-14T19:27:53.531388Z",
     "start_time": "2019-12-14T19:27:51.735579Z"
    }
   },
   "outputs": [],
   "source": [
    "#Load model and vectors\n",
    "input_length = 150\n",
    "fourgram_model = Word2Vec.load('/home/rubiales/PycharmProjects/pycharm/NLP/ignore_files/w2v_fourgram.model')\n",
    "X_pad = pd.read_csv('/home/rubiales/PycharmProjects/pycharm/NLP/ignore_files/w2v_padaseq.csv')\n",
    "X_pad.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "\n",
    "X_pad_test = pd.read_csv('/home/rubiales/PycharmProjects/pycharm/NLP/ignore_files/w2v_padaseq.csv')\n",
    "X_pad_test.drop(columns=['Unnamed: 0'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train glove2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T19:33:45.152891Z",
     "start_time": "2019-12-03T19:33:44.831709Z"
    }
   },
   "outputs": [],
   "source": [
    "glove = Glove(no_components=embedding_size, learning_rate=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-22T17:50:12.870942Z",
     "start_time": "2019-10-22T17:50:12.848130Z"
    }
   },
   "outputs": [],
   "source": [
    "#no_components: the dimension of the output vector generated by glove\n",
    "glove.fit(fourgram_model, epochs=30, no_threads=4, verbose = True)\n",
    "glove.add_dictionary(fourgram_model.wv.vocab)\n",
    "glove.save('/home/alberto/Escritorio/pycharm/NLP/ignore_files/glove.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#después de esto tengo que aplicar el resto de pasos que hic e en word2vect, es decir, ver similaridades y vectorizar los datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre_trained glove2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-04T05:28:07.290788Z",
     "start_time": "2019-12-04T05:27:56.945733Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('/home/alberto/Escritorio/pycharm/NLP/ignore_files/glove.840B.300d.txt') as f:\n",
    "    content = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-04T05:32:01.663807Z",
     "start_time": "2019-12-04T05:28:17.798248Z"
    }
   },
   "outputs": [],
   "source": [
    "embeddings_index = {}\n",
    "\n",
    "for line in tqdm.tqdm_notebook(content):\n",
    "    values = line.split(' ')\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-04T05:37:24.942200Z",
     "start_time": "2019-12-04T05:37:24.937864Z"
    }
   },
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = 150\n",
    "EMBEDDING_DIM = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-04T05:37:28.574615Z",
     "start_time": "2019-12-04T05:37:28.311202Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#aquí lo que hacemos es quitar las palabras que no están en nuestro dataset\n",
    "embedding_matrix = np.zeros((len(fourgram_model.wv.vocab) + 1, EMBEDDING_DIM))\n",
    "suma = 0\n",
    "for word, i in tqdm.notebook.tqdm(zip(fourgram_model.wv.vocab.keys(), range(len(fourgram_model.wv.vocab.keys())))):\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i-suma] = embedding_vector\n",
    "    else:\n",
    "        suma += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-14T19:28:04.382176Z",
     "start_time": "2019-12-14T19:28:03.504611Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 150, 256)          32472064  \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, 150, 64)           49216     \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 37, 64)            0         \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 256)               197632    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 64)                16448     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 32,735,425\n",
      "Trainable params: 263,361\n",
      "Non-trainable params: 32,472,064\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# import tensorflow as tf\n",
    "# with tf.device('/gpu:1'):\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim = fourgram_model.wv.vectors.shape[0],\n",
    "                   output_dim = fourgram_model.wv.vectors.shape[1],\n",
    "                   input_length = input_length,\n",
    "                   weights = [fourgram_model.wv.vectors],\n",
    "                   trainable = False))\n",
    "# model.add(Dropout(0.2))\n",
    "model.add(Conv1D(64, 3,\n",
    "                 activation='relu', padding='same'))\n",
    "model.add(MaxPool1D(pool_size=4))\n",
    "model.add(Bidirectional(LSTM(128, recurrent_dropout=0.1)))\n",
    "model.add(Dropout(0.20))\n",
    "model.add(Dense(64))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 150, 256)          32472064  \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 150, 64)           49216     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 37, 64)            0         \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 256)               197632    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                16448     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 32,735,425\n",
      "Trainable params: 263,361\n",
      "Non-trainable params: 32,472,064\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "mirrored_strategy = tf.distribute.MirroredStrategy()\n",
    "with mirrored_strategy.scope():\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim = fourgram_model.wv.vectors.shape[0],\n",
    "                       output_dim = fourgram_model.wv.vectors.shape[1],\n",
    "                       input_length = input_length,\n",
    "                       weights = [fourgram_model.wv.vectors],\n",
    "                       trainable = False))\n",
    "    # model.add(Dropout(0.2))\n",
    "    model.add(Conv1D(64, 3,\n",
    "                     activation='relu', padding='same'))\n",
    "    model.add(MaxPool1D(pool_size=4))\n",
    "    model.add(Bidirectional(LSTM(128, recurrent_dropout=0.1)))\n",
    "    model.add(Dropout(0.20))\n",
    "    model.add(Dense(64))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-14T19:29:37.732252Z",
     "start_time": "2019-12-14T19:28:06.874095Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 75000 samples\n",
      "Epoch 1/2000\n",
      "INFO:tensorflow:batch_all_reduce: 12 all-reduces with algorithm = nccl, num_packs = 1, agg_small_grads_max_bytes = 0 and agg_small_grads_max_group = 10\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:batch_all_reduce: 12 all-reduces with algorithm = nccl, num_packs = 1, agg_small_grads_max_bytes = 0 and agg_small_grads_max_group = 10\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "75000/75000 [==============================] - 12s 159us/sample - loss: 0.7212 - accuracy: 0.5112\n",
      "Epoch 2/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.6817 - accuracy: 0.5576\n",
      "Epoch 3/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.6409 - accuracy: 0.6326\n",
      "Epoch 4/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.5790 - accuracy: 0.6958\n",
      "Epoch 5/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.5437 - accuracy: 0.7274\n",
      "Epoch 6/2000\n",
      "75000/75000 [==============================] - 2s 26us/sample - loss: 0.5201 - accuracy: 0.7435\n",
      "Epoch 7/2000\n",
      "75000/75000 [==============================] - 2s 26us/sample - loss: 0.4938 - accuracy: 0.7629\n",
      "Epoch 8/2000\n",
      "75000/75000 [==============================] - 2s 26us/sample - loss: 0.4936 - accuracy: 0.7623\n",
      "Epoch 9/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.4790 - accuracy: 0.7722\n",
      "Epoch 10/2000\n",
      "75000/75000 [==============================] - 2s 26us/sample - loss: 0.4595 - accuracy: 0.7837\n",
      "Epoch 11/2000\n",
      "75000/75000 [==============================] - 2s 26us/sample - loss: 0.4527 - accuracy: 0.7888\n",
      "Epoch 12/2000\n",
      "75000/75000 [==============================] - 2s 26us/sample - loss: 0.4443 - accuracy: 0.7925\n",
      "Epoch 13/2000\n",
      "75000/75000 [==============================] - 2s 26us/sample - loss: 0.4329 - accuracy: 0.7982\n",
      "Epoch 14/2000\n",
      "75000/75000 [==============================] - 2s 27us/sample - loss: 0.4185 - accuracy: 0.8081\n",
      "Epoch 15/2000\n",
      "75000/75000 [==============================] - 2s 26us/sample - loss: 0.4481 - accuracy: 0.7902\n",
      "Epoch 16/2000\n",
      "75000/75000 [==============================] - 2s 26us/sample - loss: 0.4397 - accuracy: 0.7975\n",
      "Epoch 17/2000\n",
      "75000/75000 [==============================] - 2s 26us/sample - loss: 0.4108 - accuracy: 0.8123\n",
      "Epoch 18/2000\n",
      "75000/75000 [==============================] - 2s 26us/sample - loss: 0.3941 - accuracy: 0.8226\n",
      "Epoch 19/2000\n",
      "75000/75000 [==============================] - 2s 26us/sample - loss: 0.3973 - accuracy: 0.8208\n",
      "Epoch 20/2000\n",
      "75000/75000 [==============================] - 2s 26us/sample - loss: 0.3842 - accuracy: 0.8280\n",
      "Epoch 21/2000\n",
      "75000/75000 [==============================] - 2s 26us/sample - loss: 0.3821 - accuracy: 0.8288\n",
      "Epoch 22/2000\n",
      "75000/75000 [==============================] - 2s 26us/sample - loss: 0.3947 - accuracy: 0.8229\n",
      "Epoch 23/2000\n",
      "75000/75000 [==============================] - 2s 26us/sample - loss: 0.3787 - accuracy: 0.8310\n",
      "Epoch 24/2000\n",
      "75000/75000 [==============================] - 2s 26us/sample - loss: 0.3664 - accuracy: 0.8362\n",
      "Epoch 25/2000\n",
      "75000/75000 [==============================] - 2s 26us/sample - loss: 0.3936 - accuracy: 0.8224\n",
      "Epoch 26/2000\n",
      "75000/75000 [==============================] - 2s 26us/sample - loss: 0.3631 - accuracy: 0.8391\n",
      "Epoch 27/2000\n",
      "75000/75000 [==============================] - 2s 26us/sample - loss: 0.3516 - accuracy: 0.8461\n",
      "Epoch 28/2000\n",
      "75000/75000 [==============================] - 2s 26us/sample - loss: 0.3468 - accuracy: 0.8491\n",
      "Epoch 29/2000\n",
      "75000/75000 [==============================] - 2s 26us/sample - loss: 0.4121 - accuracy: 0.8148\n",
      "Epoch 30/2000\n",
      "75000/75000 [==============================] - 2s 26us/sample - loss: 0.3672 - accuracy: 0.8379\n",
      "Epoch 31/2000\n",
      "75000/75000 [==============================] - 2s 26us/sample - loss: 0.3477 - accuracy: 0.8467\n",
      "Epoch 32/2000\n",
      "75000/75000 [==============================] - 2s 26us/sample - loss: 0.3327 - accuracy: 0.8561\n",
      "Epoch 33/2000\n",
      "75000/75000 [==============================] - 2s 26us/sample - loss: 0.3240 - accuracy: 0.8619\n",
      "Epoch 34/2000\n",
      "75000/75000 [==============================] - 2s 26us/sample - loss: 0.3431 - accuracy: 0.8498\n",
      "Epoch 35/2000\n",
      "75000/75000 [==============================] - 2s 26us/sample - loss: 0.3423 - accuracy: 0.8500\n",
      "Epoch 36/2000\n",
      "75000/75000 [==============================] - 2s 26us/sample - loss: 0.3289 - accuracy: 0.8576\n",
      "Epoch 37/2000\n",
      "75000/75000 [==============================] - 2s 26us/sample - loss: 0.3196 - accuracy: 0.8629\n",
      "Epoch 38/2000\n",
      "75000/75000 [==============================] - 2s 26us/sample - loss: 0.3260 - accuracy: 0.8593\n",
      "Epoch 39/2000\n",
      "75000/75000 [==============================] - 2s 26us/sample - loss: 0.3198 - accuracy: 0.8622\n",
      "Epoch 40/2000\n",
      "75000/75000 [==============================] - 2s 26us/sample - loss: 0.3218 - accuracy: 0.8617\n",
      "Epoch 41/2000\n",
      "75000/75000 [==============================] - 2s 26us/sample - loss: 0.3246 - accuracy: 0.8596\n",
      "Epoch 42/2000\n",
      "75000/75000 [==============================] - 2s 26us/sample - loss: 0.3057 - accuracy: 0.8698\n",
      "Epoch 43/2000\n",
      "75000/75000 [==============================] - 2s 26us/sample - loss: 0.3108 - accuracy: 0.8659\n",
      "Epoch 44/2000\n",
      "75000/75000 [==============================] - 2s 26us/sample - loss: 0.3129 - accuracy: 0.8671\n",
      "Epoch 45/2000\n",
      "75000/75000 [==============================] - 2s 26us/sample - loss: 0.3056 - accuracy: 0.8696\n",
      "Epoch 46/2000\n",
      "75000/75000 [==============================] - 2s 26us/sample - loss: 0.2836 - accuracy: 0.8824\n",
      "Epoch 47/2000\n",
      "75000/75000 [==============================] - 2s 26us/sample - loss: 0.3297 - accuracy: 0.8556\n",
      "Epoch 48/2000\n",
      "75000/75000 [==============================] - 2s 26us/sample - loss: 0.2809 - accuracy: 0.8832\n",
      "Epoch 49/2000\n",
      "75000/75000 [==============================] - 2s 26us/sample - loss: 0.2851 - accuracy: 0.8802\n",
      "Epoch 50/2000\n",
      "75000/75000 [==============================] - 2s 26us/sample - loss: 0.2787 - accuracy: 0.8828\n",
      "Epoch 51/2000\n",
      "75000/75000 [==============================] - 2s 26us/sample - loss: 0.2737 - accuracy: 0.8852\n",
      "Epoch 52/2000\n",
      "75000/75000 [==============================] - 2s 26us/sample - loss: 0.2809 - accuracy: 0.8818\n",
      "Epoch 53/2000\n",
      "75000/75000 [==============================] - 2s 26us/sample - loss: 0.2916 - accuracy: 0.8769\n",
      "Epoch 54/2000\n",
      "75000/75000 [==============================] - 2s 26us/sample - loss: 0.2875 - accuracy: 0.8774\n",
      "Epoch 55/2000\n",
      "75000/75000 [==============================] - 2s 26us/sample - loss: 0.2662 - accuracy: 0.8895\n",
      "Epoch 56/2000\n",
      "75000/75000 [==============================] - 2s 26us/sample - loss: 0.2603 - accuracy: 0.8910\n",
      "Epoch 57/2000\n",
      "75000/75000 [==============================] - 2s 26us/sample - loss: 0.2549 - accuracy: 0.8950\n",
      "Epoch 58/2000\n",
      "75000/75000 [==============================] - 2s 26us/sample - loss: 0.2524 - accuracy: 0.8946\n",
      "Epoch 59/2000\n",
      "75000/75000 [==============================] - 2s 26us/sample - loss: 0.2415 - accuracy: 0.9011\n",
      "Epoch 60/2000\n",
      "75000/75000 [==============================] - 2s 26us/sample - loss: 0.2793 - accuracy: 0.8812\n",
      "Epoch 61/2000\n",
      "75000/75000 [==============================] - 2s 26us/sample - loss: 0.2773 - accuracy: 0.8831\n",
      "Epoch 62/2000\n",
      "75000/75000 [==============================] - 2s 26us/sample - loss: 0.2378 - accuracy: 0.9033\n",
      "Epoch 63/2000\n",
      "75000/75000 [==============================] - 2s 26us/sample - loss: 0.2220 - accuracy: 0.9106\n",
      "Epoch 64/2000\n",
      "75000/75000 [==============================] - 2s 26us/sample - loss: 0.2724 - accuracy: 0.8846\n",
      "Epoch 65/2000\n",
      "75000/75000 [==============================] - 2s 26us/sample - loss: 0.2624 - accuracy: 0.8896\n",
      "Epoch 66/2000\n",
      "75000/75000 [==============================] - 2s 26us/sample - loss: 0.2314 - accuracy: 0.9051\n",
      "Epoch 67/2000\n",
      "75000/75000 [==============================] - 2s 26us/sample - loss: 0.2279 - accuracy: 0.9061\n",
      "Epoch 68/2000\n",
      "75000/75000 [==============================] - 2s 26us/sample - loss: 0.2338 - accuracy: 0.9033\n",
      "Epoch 69/2000\n",
      "75000/75000 [==============================] - 2s 26us/sample - loss: 0.2199 - accuracy: 0.9099\n",
      "Epoch 70/2000\n",
      "75000/75000 [==============================] - 2s 26us/sample - loss: 0.2026 - accuracy: 0.9185\n",
      "Epoch 71/2000\n",
      "75000/75000 [==============================] - 2s 26us/sample - loss: 0.1974 - accuracy: 0.9216\n",
      "Epoch 72/2000\n",
      "75000/75000 [==============================] - 2s 26us/sample - loss: 0.2230 - accuracy: 0.9077\n",
      "Epoch 73/2000\n",
      "75000/75000 [==============================] - 2s 26us/sample - loss: 0.2060 - accuracy: 0.9169\n",
      "Epoch 74/2000\n",
      "75000/75000 [==============================] - 2s 26us/sample - loss: 0.1894 - accuracy: 0.9244\n",
      "Epoch 75/2000\n",
      "75000/75000 [==============================] - 2s 26us/sample - loss: 0.1772 - accuracy: 0.9311\n",
      "Epoch 76/2000\n",
      "75000/75000 [==============================] - 2s 26us/sample - loss: 0.1861 - accuracy: 0.9258\n",
      "Epoch 77/2000\n",
      "75000/75000 [==============================] - 2s 26us/sample - loss: 0.1906 - accuracy: 0.9231\n",
      "Epoch 78/2000\n",
      "75000/75000 [==============================] - 2s 26us/sample - loss: 0.1940 - accuracy: 0.9214\n",
      "Epoch 79/2000\n",
      "75000/75000 [==============================] - 2s 26us/sample - loss: 0.1992 - accuracy: 0.9191\n",
      "Epoch 80/2000\n",
      "75000/75000 [==============================] - 2s 26us/sample - loss: 0.1991 - accuracy: 0.9183\n",
      "Epoch 81/2000\n",
      "75000/75000 [==============================] - 2s 26us/sample - loss: 0.1759 - accuracy: 0.9298\n",
      "Epoch 82/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.1556 - accuracy: 0.9394\n",
      "Epoch 83/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.1554 - accuracy: 0.9403\n",
      "Epoch 84/2000\n",
      "75000/75000 [==============================] - 2s 26us/sample - loss: 0.1587 - accuracy: 0.9387\n",
      "Epoch 85/2000\n",
      "75000/75000 [==============================] - 2s 26us/sample - loss: 0.2073 - accuracy: 0.9135\n",
      "Epoch 86/2000\n",
      "75000/75000 [==============================] - 2s 26us/sample - loss: 0.1588 - accuracy: 0.9385\n",
      "Epoch 87/2000\n",
      "75000/75000 [==============================] - 2s 26us/sample - loss: 0.1361 - accuracy: 0.9480\n",
      "Epoch 88/2000\n",
      "75000/75000 [==============================] - 2s 26us/sample - loss: 0.1509 - accuracy: 0.9413\n",
      "Epoch 89/2000\n",
      "75000/75000 [==============================] - 2s 26us/sample - loss: 0.1328 - accuracy: 0.9501\n",
      "Epoch 90/2000\n",
      "75000/75000 [==============================] - 2s 26us/sample - loss: 0.1230 - accuracy: 0.9536\n",
      "Epoch 91/2000\n",
      "75000/75000 [==============================] - 2s 26us/sample - loss: 0.1584 - accuracy: 0.9363\n",
      "Epoch 92/2000\n",
      "75000/75000 [==============================] - 2s 26us/sample - loss: 0.1266 - accuracy: 0.9527\n",
      "Epoch 93/2000\n",
      "75000/75000 [==============================] - 2s 26us/sample - loss: 0.1150 - accuracy: 0.9579\n",
      "Epoch 94/2000\n",
      "75000/75000 [==============================] - 2s 26us/sample - loss: 0.3145 - accuracy: 0.8695\n",
      "Epoch 95/2000\n",
      "75000/75000 [==============================] - 2s 26us/sample - loss: 0.1931 - accuracy: 0.9237\n",
      "Epoch 96/2000\n",
      "75000/75000 [==============================] - 2s 26us/sample - loss: 0.1449 - accuracy: 0.9452\n",
      "Epoch 97/2000\n",
      "75000/75000 [==============================] - 2s 26us/sample - loss: 0.1240 - accuracy: 0.9537\n",
      "Epoch 98/2000\n",
      "75000/75000 [==============================] - 2s 26us/sample - loss: 0.1120 - accuracy: 0.9588\n",
      "Epoch 99/2000\n",
      "75000/75000 [==============================] - 2s 26us/sample - loss: 0.1053 - accuracy: 0.9609\n",
      "Epoch 100/2000\n",
      "75000/75000 [==============================] - 2s 26us/sample - loss: 0.1205 - accuracy: 0.9540\n",
      "Epoch 101/2000\n",
      "75000/75000 [==============================] - 2s 26us/sample - loss: 0.1171 - accuracy: 0.9550\n",
      "Epoch 102/2000\n",
      "75000/75000 [==============================] - 2s 26us/sample - loss: 0.0943 - accuracy: 0.9658\n",
      "Epoch 103/2000\n",
      "75000/75000 [==============================] - 2s 26us/sample - loss: 0.0959 - accuracy: 0.9648\n",
      "Epoch 104/2000\n",
      "75000/75000 [==============================] - 2s 26us/sample - loss: 0.0924 - accuracy: 0.9658\n",
      "Epoch 105/2000\n",
      "75000/75000 [==============================] - 2s 26us/sample - loss: 0.1060 - accuracy: 0.9604\n",
      "Epoch 106/2000\n",
      "75000/75000 [==============================] - 2s 26us/sample - loss: 0.4910 - accuracy: 0.8093\n",
      "Epoch 107/2000\n",
      "75000/75000 [==============================] - 2s 26us/sample - loss: 0.2829 - accuracy: 0.8804\n",
      "Epoch 108/2000\n",
      "75000/75000 [==============================] - 2s 26us/sample - loss: 0.2366 - accuracy: 0.9026\n",
      "Epoch 109/2000\n",
      "75000/75000 [==============================] - 2s 26us/sample - loss: 0.1892 - accuracy: 0.9240\n",
      "Epoch 110/2000\n",
      "75000/75000 [==============================] - 2s 26us/sample - loss: 0.1597 - accuracy: 0.9361\n",
      "Epoch 111/2000\n",
      "75000/75000 [==============================] - 2s 26us/sample - loss: 0.1438 - accuracy: 0.9438\n",
      "Epoch 112/2000\n",
      "75000/75000 [==============================] - 2s 26us/sample - loss: 0.1491 - accuracy: 0.9400\n",
      "Epoch 113/2000\n",
      "75000/75000 [==============================] - 2s 26us/sample - loss: 0.1310 - accuracy: 0.9495\n",
      "Epoch 114/2000\n",
      "75000/75000 [==============================] - 2s 26us/sample - loss: 0.1093 - accuracy: 0.9589\n",
      "Epoch 115/2000\n",
      "75000/75000 [==============================] - 2s 26us/sample - loss: 0.1199 - accuracy: 0.9528\n",
      "Epoch 116/2000\n",
      "75000/75000 [==============================] - 2s 26us/sample - loss: 0.1005 - accuracy: 0.9623\n",
      "Epoch 117/2000\n",
      "75000/75000 [==============================] - 2s 26us/sample - loss: 0.0916 - accuracy: 0.9665\n",
      "Epoch 118/2000\n",
      "75000/75000 [==============================] - 2s 26us/sample - loss: 0.0821 - accuracy: 0.9702\n",
      "Epoch 119/2000\n",
      "75000/75000 [==============================] - 2s 26us/sample - loss: 0.0797 - accuracy: 0.9713\n",
      "Epoch 120/2000\n",
      "75000/75000 [==============================] - 2s 26us/sample - loss: 0.0804 - accuracy: 0.9704\n",
      "Epoch 121/2000\n",
      "75000/75000 [==============================] - 2s 26us/sample - loss: 0.0852 - accuracy: 0.9684\n",
      "Epoch 122/2000\n",
      "75000/75000 [==============================] - 2s 26us/sample - loss: 0.0813 - accuracy: 0.9706\n",
      "Epoch 123/2000\n",
      "75000/75000 [==============================] - 2s 26us/sample - loss: 0.0766 - accuracy: 0.9716\n",
      "Epoch 124/2000\n",
      "75000/75000 [==============================] - 2s 26us/sample - loss: 0.0726 - accuracy: 0.9733\n",
      "Epoch 125/2000\n",
      "75000/75000 [==============================] - 2s 26us/sample - loss: 0.0718 - accuracy: 0.9734\n",
      "Epoch 126/2000\n",
      "75000/75000 [==============================] - 2s 26us/sample - loss: 0.0778 - accuracy: 0.9714\n",
      "Epoch 127/2000\n",
      "75000/75000 [==============================] - 2s 26us/sample - loss: 0.0657 - accuracy: 0.9760\n",
      "Epoch 128/2000\n",
      "75000/75000 [==============================] - 2s 26us/sample - loss: 0.0718 - accuracy: 0.9730\n",
      "Epoch 129/2000\n",
      "75000/75000 [==============================] - 2s 26us/sample - loss: 0.3129 - accuracy: 0.8977\n",
      "Epoch 130/2000\n",
      "75000/75000 [==============================] - 2s 26us/sample - loss: 0.3433 - accuracy: 0.8524\n",
      "Epoch 131/2000\n",
      "75000/75000 [==============================] - 2s 26us/sample - loss: 0.2094 - accuracy: 0.9144\n",
      "Epoch 132/2000\n",
      "75000/75000 [==============================] - 2s 26us/sample - loss: 0.1533 - accuracy: 0.9424\n",
      "Epoch 133/2000\n",
      "75000/75000 [==============================] - 2s 26us/sample - loss: 0.1155 - accuracy: 0.9575\n",
      "Epoch 134/2000\n",
      "75000/75000 [==============================] - 2s 27us/sample - loss: 0.0931 - accuracy: 0.9663\n",
      "Epoch 135/2000\n",
      "24000/75000 [========>.....................] - ETA: 1s - loss: 0.0788 - accuracy: 0.9712"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-b9813bc9f563>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# with tf.device('/gpu:1'):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX_pad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentiment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m6000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m#Write X_pad.values if it's trained.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pycharm/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    727\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 728\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    729\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    730\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/miniconda3/envs/pycharm/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[1;32m    322\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 324\u001b[0;31m                 total_epochs=epochs)\n\u001b[0m\u001b[1;32m    325\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pycharm/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[0;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[1;32m    121\u001b[0m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[1;32m    122\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0;31m# TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pycharm/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[0;34m(input_fn)\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0;31m# `numpy` translates Tensors to values in Eager mode.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[0;32m---> 86\u001b[0;31m                               distributed_function(input_fn))\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pycharm/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_counter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalled_without_tracing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pycharm/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    485\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 487\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    488\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pycharm/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1821\u001b[0m     \u001b[0;34m\"\"\"Calls a graph function specialized to the inputs.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1822\u001b[0m     \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1823\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1824\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1825\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pycharm/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1139\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1140\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1141\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1143\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pycharm/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1222\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1223\u001b[0m       flat_outputs = forward_function.call(\n\u001b[0;32m-> 1224\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1225\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1226\u001b[0m       \u001b[0mgradient_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_delayed_rewrite_functions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pycharm/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    509\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"executor_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"config_proto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 511\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    512\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/miniconda3/envs/pycharm/lib/python3.7/site-packages/tensorflow_core/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[1;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# with tf.device('/gpu:1'):\n",
    "model.fit(x=X_pad.values, y=df_train.sentiment.values, batch_size=6000, epochs=2000)\n",
    "#Write X_pad.values if it's trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-08T20:14:04.619988Z",
     "start_time": "2019-12-08T20:14:04.615995Z"
    }
   },
   "outputs": [],
   "source": [
    "gpus[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-08T12:56:26.804171Z",
     "start_time": "2019-12-08T12:56:25.606Z"
    }
   },
   "outputs": [],
   "source": [
    "model.save('/home/alberto/Escritorio/pycharm/NLP/ignore_files/sentyment_model_w2v_trained.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('/home/alberto/Escritorio/pycharm/NLP/ignore_files/sentyment_model_w2v_trained.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-01T18:07:47.170335Z",
     "start_time": "2019-12-01T18:06:45.121236Z"
    }
   },
   "outputs": [],
   "source": [
    "#Submit the results to kaggle\n",
    "X_submit = model.predict_classes(X_pad_test.values)\n",
    "submission = pd.merge(df_test.id, pd.DataFrame(X_submit, columns=['sentiment']), left_index=True, right_index=True)\n",
    "submission.to_csv('submission_w2v', index=False)\n",
    "#Kaggle give us a 99.24% of accuracy wich is great"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Glove2vec trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-01T19:46:24.747204Z",
     "start_time": "2019-12-01T19:46:21.131692Z"
    }
   },
   "outputs": [],
   "source": [
    "#esto hay que cambiar la capa de embedding por la de glove\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim = fourgram_model.wv.vectors.shape[0],\n",
    "                   output_dim = fourgram_model.wv.vectors.shape[1],\n",
    "                   input_length = input_length,\n",
    "                   weights = [fourgram_model.wv.vectors],\n",
    "                   trainable = False))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Conv1D(64, 3, activation='relu', padding='same'))\n",
    "model.add(MaxPool1D(pool_size=4))\n",
    "model.add(Bidirectional(LSTM(128, recurrent_dropout=0.1)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(64))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Glove2vec Pre trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-04T05:37:40.830965Z",
     "start_time": "2019-12-04T05:37:38.101384Z"
    }
   },
   "outputs": [],
   "source": [
    "#esto hay que cambiar la capa de embedding por la de glove\n",
    "model_glove = Sequential()\n",
    "model_glove.add(Embedding(input_dim = len(fourgram_model.wv.vectors) + 1,\n",
    "                   output_dim = EMBEDDING_DIM,\n",
    "                   input_length = input_length,\n",
    "                   weights = [embedding_matrix],\n",
    "                   trainable = False))\n",
    "# model_glove.add(Dropout(0.2))\n",
    "model_glove.add(Conv1D(64, 3, activation='relu', padding='same'))\n",
    "model_glove.add(MaxPool1D(pool_size=4))\n",
    "model_glove.add(Bidirectional(LSTM(128, recurrent_dropout=0.1)))\n",
    "model_glove.add(Dropout(0.25))\n",
    "model_glove.add(Dense(64))\n",
    "model_glove.add(Dropout(0.3))\n",
    "model_glove.add(Dense(1, activation='sigmoid'))\n",
    "model_glove.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model_glove.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-04T06:07:01.175478Z",
     "start_time": "2019-12-04T05:37:44.889131Z"
    }
   },
   "outputs": [],
   "source": [
    "model_glove.fit(x=X_pad.values, y=df_train.sentiment.values, batch_size=100, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-04T06:19:55.163894Z",
     "start_time": "2019-12-04T06:19:54.630978Z"
    }
   },
   "outputs": [],
   "source": [
    "model_glove.save('/home/alberto/Escritorio/pycharm/NLP/ignore_files/sentyment_model_G2V_trained.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-04T06:19:58.820007Z",
     "start_time": "2019-12-04T06:19:58.723322Z"
    }
   },
   "outputs": [],
   "source": [
    "model_glove.load_weights('/home/alberto/Escritorio/pycharm/NLP/ignore_files/sentyment_model_G2V_trained.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-04T06:21:04.952484Z",
     "start_time": "2019-12-04T06:20:01.884262Z"
    }
   },
   "outputs": [],
   "source": [
    "#Submit the results to kaggle\n",
    "X_submit = model_glove.predict_classes(X_pad_test.values)\n",
    "submission = pd.merge(df_test.id, pd.DataFrame(X_submit, columns=['sentiment']), left_index=True, right_index=True)\n",
    "submission.to_csv('submission_G2V', index=False)\n",
    "#Kaggle give us a 98% of accuracy wich is great"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# otro modelo a probar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#este funciona más rápido y casi con el mismo accuracy\n",
    "model_glove = Sequential()\n",
    "model_glove.add(Embedding(input_dim = fourgram_model.wv.vectors.shape[0],\n",
    "                   output_dim = fourgram_model.wv.vectors.shape[1],\n",
    "                   input_length = input_length,\n",
    "                   weights = [fourgram_model.wv.vectors],\n",
    "                   trainable = False))\n",
    "model_glove.add(Dropout(0.2))\n",
    "model_glove.add(Conv1D(64, 3, activation='relu', padding='same'))\n",
    "model_glove.add(MaxPool1D(pool_size=4))\n",
    "model_glove.add(Bidirectional(LSTM(128, recurrent_dropout=0.1)))\n",
    "model_glove.add(Dropout(0.25))\n",
    "model_glove.add(Dense(64))\n",
    "model_glove.add(Dropout(0.3))\n",
    "model_glove.add(Dense(1, activation='sigmoid'))\n",
    "model_glove.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_glove.fit(x=X_pad, y=df_train.sentiment.values, batch_size=100, epochs=25)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "214.162px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "465.412px",
    "left": "1745.91px",
    "right": "20px",
    "top": "121.94px",
    "width": "471.293px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-14T20:36:55.418287Z",
     "start_time": "2019-12-14T20:36:55.410615Z"
    }
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import numba\n",
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import nltk\n",
    "from dask.multiprocessing import get\n",
    "from gensim.models import Word2Vec, Phrases\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Bidirectional, Dropout, Dense, Attention, Embedding, Conv1D, MaxPool1D\n",
    "import tqdm\n",
    "from multiprocessing import cpu_count\n",
    "import tensorflow as tf\n",
    "\n",
    "#Configuraciones \n",
    "from tensorflow.compat.v1 import ConfigProto\n",
    "from tensorflow.compat.v1 import InteractiveSession\n",
    "\n",
    "config = ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = InteractiveSession(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-14T19:27:14.358538Z",
     "start_time": "2019-12-14T19:27:14.353691Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset_1_zip = zipfile.ZipFile('/home/rubiales/PycharmProjects/pycharm/NLP/ignore_files/IMDB Dataset.csv.zip', 'r')\n",
    "dataset_1_csv = dataset_1_zip.open('IMDB Dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-14T19:27:16.012360Z",
     "start_time": "2019-12-14T19:27:15.360531Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamaño del dataframe:  (50000, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  One of the other reviewers has mentioned that ...          1\n",
       "1  A wonderful little production. <br /><br />The...          1\n",
       "2  I thought this was a wonderful way to spend ti...          1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_1_train = pd.read_csv(dataset_1_csv)\n",
    "#positive and negative sentimen to categorical\n",
    "df_1_train['sentiment'] = pd.Categorical(df_1_train.sentiment).codes\n",
    "print('Tamaño del dataframe: ', df_1_train.shape)\n",
    "df_1_train[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-14T19:27:16.856622Z",
     "start_time": "2019-12-14T19:27:16.848117Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset_2_zip = zipfile.ZipFile('/home/rubiales/PycharmProjects/pycharm/NLP/ignore_files/word2vec-nlp-tutorial.zip')\n",
    "dataset_2_csv_train = dataset_2_zip.open('labeledTrainData.tsv')\n",
    "dataset_2_csv_test = dataset_2_zip.open('testData.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-14T19:27:18.530835Z",
     "start_time": "2019-12-14T19:27:17.848390Z"
    }
   },
   "outputs": [],
   "source": [
    "df_2_train = pd.read_csv(dataset_2_csv_train, sep='\\t')\n",
    "df_test = pd.read_csv(dataset_2_csv_test, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-14T19:27:18.692240Z",
     "start_time": "2019-12-14T19:27:18.681195Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              review  sentiment\n",
      "0  One of the other reviewers has mentioned that ...          1\n",
      "1  A wonderful little production. <br /><br />The...          1\n",
      "2  I thought this was a wonderful way to spend ti...          1\n",
      "3  Basically there's a family where a little boy ...          0\n",
      "4  Petter Mattei's \"Love in the Time of Money\" is...          1\n",
      "       id  sentiment                                             review\n",
      "0  5814_8          1  With all this stuff going down at the moment w...\n",
      "1  2381_9          1  \\The Classic War of the Worlds\\\" by Timothy Hi...\n",
      "2  7759_3          0  The film starts with a manager (Nicholas Bell)...\n",
      "3  3630_4          0  It must be assumed that those who praised this...\n",
      "4  9495_8          1  Superbly trashy and wondrously unpretentious 8...\n",
      "         id                                             review\n",
      "0  12311_10  Naturally in a film who's main themes are of m...\n",
      "1    8348_2  This movie is a disaster within a disaster fil...\n",
      "2    5828_4  All in all, this is a movie for kids. We saw i...\n",
      "3    7186_2  Afraid of the Dark left me with the impression...\n",
      "4   12128_7  A very accurate depiction of small time mob li...\n"
     ]
    }
   ],
   "source": [
    "print(df_1_train.head())\n",
    "print(df_2_train.head())\n",
    "print(df_test.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that our first dataset doesn't has the column \"id\" so we will drop this column fromo our datasets and let concat the dataframe train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-14T19:27:27.758935Z",
     "start_time": "2019-12-14T19:27:27.681515Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "columnas train: Index(['review', 'sentiment'], dtype='object')\n",
      "columnas test: Index(['id', 'review'], dtype='object')\n",
      "Shape train: (75000, 2)\n",
      "Shape test: (25000, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  One of the other reviewers has mentioned that ...          1\n",
       "1  A wonderful little production. <br /><br />The...          1\n",
       "2  I thought this was a wonderful way to spend ti...          1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df_test.drop(columns='id', inplace=True)\n",
    "df_train = pd.concat([df_1_train, df_2_train[['sentiment', 'review']]], axis=0, sort=False)\n",
    "df_train.reset_index(drop=True, inplace=True)\n",
    "print('columnas train:', df_train.columns)\n",
    "print('columnas test:', df_test.columns)\n",
    "print('Shape train:', df_train.shape)\n",
    "print('Shape test:', df_test.shape)\n",
    "df_train.review = df_train.review.astype(str)\n",
    "df_train[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-30T10:38:43.898379Z",
     "start_time": "2019-11-30T10:38:43.894932Z"
    }
   },
   "source": [
    "## Pass everything to Dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-14T19:27:28.579266Z",
     "start_time": "2019-12-14T19:27:28.576601Z"
    }
   },
   "outputs": [],
   "source": [
    "#count number of threads\n",
    "cores = cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-14T19:27:29.467682Z",
     "start_time": "2019-12-14T19:27:29.169192Z"
    }
   },
   "outputs": [],
   "source": [
    "#Create 2 empty columns for dask text processing\n",
    "df_train['procesed'] = ''\n",
    "df_test['procesed'] = ''\n",
    "dask_train = dd.from_pandas(df_train, npartitions=cores)\n",
    "dask_test = dd.from_pandas(df_test, npartitions=cores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Pre - Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-14T19:27:30.788211Z",
     "start_time": "2019-12-14T19:27:30.777174Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#Remove html\n",
    "def parse_html(nlp):\n",
    "    return BeautifulSoup(nlp).get_text()\n",
    "\n",
    "#Remove non words\n",
    "def remove_nonwords(nlp):\n",
    "    return re.sub(\"[^a-zA-Z]\", \" \", nlp)\n",
    "\n",
    "#Lower all text\n",
    "def lower(nlp):\n",
    "    return nlp.lower()\n",
    "\n",
    "#remove stopwords\n",
    "def remove_stopwords(nlp):\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "    splited = nlp.split()\n",
    "    removed = [item for item in splited if item not in stopwords]\n",
    "    joined = ' '.join(removed)\n",
    "    return joined\n",
    "\n",
    "#tokenize text\n",
    "def own_tokenizer(nlp):\n",
    "    return nltk.word_tokenize(nlp)\n",
    "\n",
    "#lematize words\n",
    "def own_lemmatizer(nlp):\n",
    "    lemmatizer = nltk.stem.WordNetLemmatizer()            \n",
    "    lemmas = list(map(lemmatizer.lemmatize, nlp))\n",
    "    return lemmas\n",
    "\n",
    "#a fuction that group all the preprocess functions\n",
    "def df_clean(df_train):\n",
    "    df_train['procesed'] = df_train.review.map(parse_html).map(remove_nonwords).map(lower).map(remove_stopwords).map(\n",
    "    own_tokenizer).map(own_lemmatizer)\n",
    "    return df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-14T19:27:43.167783Z",
     "start_time": "2019-12-14T19:27:31.258453Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>procesed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[one, reviewer, mentioned, watching, oz, episo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>1</td>\n",
       "      <td>[wonderful, little, production, filming, techn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>1</td>\n",
       "      <td>[thought, wonderful, way, spend, time, hot, su...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[basically, family, little, boy, jake, think, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>1</td>\n",
       "      <td>[petter, mattei, love, time, money, visually, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74995</th>\n",
       "      <td>It seems like more consideration has gone into...</td>\n",
       "      <td>0</td>\n",
       "      <td>[seems, like, consideration, gone, imdb, revie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74996</th>\n",
       "      <td>I don't believe they made this film. Completel...</td>\n",
       "      <td>0</td>\n",
       "      <td>[believe, made, film, completely, unnecessary,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74997</th>\n",
       "      <td>Guy is a loser. Can't get girls, needs to buil...</td>\n",
       "      <td>0</td>\n",
       "      <td>[guy, loser, get, girl, need, build, picked, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74998</th>\n",
       "      <td>This 30 minute documentary Buñuel made in the ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[minute, documentary, bu, uel, made, early, on...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74999</th>\n",
       "      <td>I saw this movie as a child and it broke my he...</td>\n",
       "      <td>1</td>\n",
       "      <td>[saw, movie, child, broke, heart, story, unfin...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>75000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review  sentiment  \\\n",
       "0      One of the other reviewers has mentioned that ...          1   \n",
       "1      A wonderful little production. <br /><br />The...          1   \n",
       "2      I thought this was a wonderful way to spend ti...          1   \n",
       "3      Basically there's a family where a little boy ...          0   \n",
       "4      Petter Mattei's \"Love in the Time of Money\" is...          1   \n",
       "...                                                  ...        ...   \n",
       "74995  It seems like more consideration has gone into...          0   \n",
       "74996  I don't believe they made this film. Completel...          0   \n",
       "74997  Guy is a loser. Can't get girls, needs to buil...          0   \n",
       "74998  This 30 minute documentary Buñuel made in the ...          0   \n",
       "74999  I saw this movie as a child and it broke my he...          1   \n",
       "\n",
       "                                                procesed  \n",
       "0      [one, reviewer, mentioned, watching, oz, episo...  \n",
       "1      [wonderful, little, production, filming, techn...  \n",
       "2      [thought, wonderful, way, spend, time, hot, su...  \n",
       "3      [basically, family, little, boy, jake, think, ...  \n",
       "4      [petter, mattei, love, time, money, visually, ...  \n",
       "...                                                  ...  \n",
       "74995  [seems, like, consideration, gone, imdb, revie...  \n",
       "74996  [believe, made, film, completely, unnecessary,...  \n",
       "74997  [guy, loser, get, girl, need, build, picked, s...  \n",
       "74998  [minute, documentary, bu, uel, made, early, on...  \n",
       "74999  [saw, movie, child, broke, heart, story, unfin...  \n",
       "\n",
       "[75000 rows x 3 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dask_train_processed = dask_train.map_partitions(df_clean, meta=df_train)\n",
    "dask_test_processed = dask_test.map_partitions(df_clean, meta=df_test)\n",
    "preprocessed_train = dask_train_processed.compute(scheduler='processes')\n",
    "preprocessed_test = dask_test_processed.compute(scheduler='processes')\n",
    "preprocessed_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-16T08:56:15.970836Z",
     "start_time": "2019-10-16T08:56:15.961833Z"
    }
   },
   "source": [
    "# mini-EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-14T19:27:43.170978Z",
     "start_time": "2019-12-14T19:27:32.223Z"
    }
   },
   "outputs": [],
   "source": [
    "print('Balance of the classes in train:',  preprocessed_train.sentiment.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-14T19:27:43.172604Z",
     "start_time": "2019-12-14T19:27:32.831Z"
    }
   },
   "outputs": [],
   "source": [
    "lenth = preprocessed_train.procesed.map(len)\n",
    "print('Mean', lenth.mean())\n",
    "print('Median', lenth.median())\n",
    "print('Mode', lenth.mode()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing to model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-14T19:27:43.174708Z",
     "start_time": "2019-12-14T19:27:36.871Z"
    }
   },
   "outputs": [],
   "source": [
    "#Create the vocabulary in Bigrams\n",
    "bigrams = Phrases(sentences=preprocessed_train['procesed'], min_count=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-14T19:27:43.177754Z",
     "start_time": "2019-12-14T19:27:37.023Z"
    }
   },
   "outputs": [],
   "source": [
    "#Create the vocabulary in Trigrams\n",
    "trigrams = Phrases(sentences=bigrams[preprocessed_train['procesed']], min_count=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-14T19:27:43.180698Z",
     "start_time": "2019-12-14T19:27:37.368Z"
    }
   },
   "outputs": [],
   "source": [
    "#Create the vocabulary in cuatrigrams\n",
    "fourgrams = Phrases(sentences=trigrams[preprocessed_train['procesed']], min_count=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-01T10:48:09.024788Z",
     "start_time": "2019-12-01T10:43:08.407403Z"
    }
   },
   "outputs": [],
   "source": [
    "#Bigram model\n",
    "#window is the maximun distance between the current and predicted word within a sentence\n",
    "embedding_size = 256\n",
    "bigram_model = Word2Vec(sentences = bigrams[preprocessed_train['procesed']], size=embedding_size,\n",
    "                        min_count=4, window=5, workers=cores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-01T10:56:54.852152Z",
     "start_time": "2019-12-01T10:48:09.362970Z"
    }
   },
   "outputs": [],
   "source": [
    "#Trigram model\n",
    "trigrams_model = Word2Vec(sentences = trigrams[bigrams[preprocessed_train['procesed']]], size=embedding_size,\n",
    "                        min_count=3, window=5, workers=cores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-01T11:05:53.969338Z",
     "start_time": "2019-12-01T10:57:20.091938Z"
    }
   },
   "outputs": [],
   "source": [
    "#Fourgram model\n",
    "fourgram_model = Word2Vec(sentences = fourgrams[trigrams[preprocessed_train['procesed']]], size=embedding_size,\n",
    "                        min_count=3, window=5, workers=cores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-01T10:56:55.283415Z",
     "start_time": "2019-12-01T10:56:33.946Z"
    }
   },
   "source": [
    "### Test the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-01T11:07:57.969052Z",
     "start_time": "2019-12-01T11:07:57.783878Z"
    }
   },
   "outputs": [],
   "source": [
    "bigram_model.wv.most_similar('america')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-01T11:07:59.101689Z",
     "start_time": "2019-12-01T11:07:58.864109Z"
    }
   },
   "outputs": [],
   "source": [
    "trigrams_model.wv.most_similar('america')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-01T11:08:00.038992Z",
     "start_time": "2019-12-01T11:07:59.790160Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('nation', 0.8577866554260254),\n",
       " ('india', 0.8333728313446045),\n",
       " ('japan', 0.8321987390518188),\n",
       " ('australia', 0.81800776720047),\n",
       " ('germany', 0.798524022102356),\n",
       " ('united_state', 0.7973009347915649),\n",
       " ('europe', 0.795366108417511),\n",
       " ('country', 0.7930383682250977),\n",
       " ('britain', 0.7832762002944946),\n",
       " ('russia', 0.7785017490386963)]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fourgram_model.wv.most_similar('america')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('wife', 0.9080113172531128),\n",
       " ('daughter', 0.8578042984008789),\n",
       " ('mother', 0.8107233047485352),\n",
       " ('father', 0.8020291924476624),\n",
       " ('son', 0.7928421497344971),\n",
       " ('marriage', 0.7738940715789795),\n",
       " ('sister', 0.7531670331954956),\n",
       " ('boyfriend', 0.7522717714309692),\n",
       " ('married', 0.7418345212936401),\n",
       " ('spouse', 0.741738498210907)]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fourgram_model.wv.most_similar('husband')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-01T11:08:31.105824Z",
     "start_time": "2019-12-01T11:08:31.093719Z"
    }
   },
   "outputs": [],
   "source": [
    "X_data = fourgrams[trigrams[bigrams[preprocessed_train['procesed']]]]\n",
    "\n",
    "X_data_test = fourgrams[trigrams[bigrams[preprocessed_test['procesed']]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-01T11:08:33.899039Z",
     "start_time": "2019-12-01T11:08:33.891225Z"
    }
   },
   "outputs": [],
   "source": [
    "def word_2_vec(data, vocab):\n",
    "    keys = list(vocab.keys())\n",
    "    filter_unknown = lambda word: vocab.get(word, None) is not None\n",
    "    encode = lambda review: list(map(keys.index, filter(filter_unknown, review)))\n",
    "    vectorized = list(map(encode, data))\n",
    "    return vectorized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-01T12:04:26.061457Z",
     "start_time": "2019-12-01T11:08:39.271314Z"
    }
   },
   "outputs": [],
   "source": [
    "word_vector = word_2_vec(X_data, fourgram_model.wv.vocab)\n",
    "word_vector_test = word_2_vec(X_data_test, fourgram_model.wv.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-08T13:15:15.204577Z",
     "start_time": "2019-12-08T13:15:15.127844Z"
    }
   },
   "outputs": [],
   "source": [
    "input_length = 150\n",
    "X_pad = pad_sequences(sequences=word_vector, maxlen=input_length, padding='post')\n",
    "X_pad_test = pad_sequences(sequences=word_vector_test, maxlen=input_length, padding='post')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Guardamos los modelos y variables necesarias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-27T16:41:56.762348Z",
     "start_time": "2019-10-27T16:41:56.746792Z"
    }
   },
   "outputs": [],
   "source": [
    "#Save the models and text transformations\n",
    "# fourgram_model.save('/home/alberto/Escritorio/pycharm/NLP/ignore_files/w2v_fourgram.model')\n",
    "# pd.DataFrame(X_pad).to_csv('/home/alberto/Escritorio/pycharm/NLP/ignore_files/w2v_padaseq.csv')\n",
    "# pd.DataFrame(X_pad_test).to_csv('/home/alberto/Escritorio/pycharm/NLP/ignore_files/w2v_padaseq_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-14T19:27:53.531388Z",
     "start_time": "2019-12-14T19:27:51.735579Z"
    }
   },
   "outputs": [],
   "source": [
    "#Load model and vectors\n",
    "input_length = 150\n",
    "fourgram_model = Word2Vec.load('/home/rubiales/PycharmProjects/pycharm/NLP/ignore_files/w2v_fourgram.model')\n",
    "X_pad = pd.read_csv('/home/rubiales/PycharmProjects/pycharm/NLP/ignore_files/w2v_padaseq.csv')\n",
    "X_pad.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "\n",
    "X_pad_test = pd.read_csv('/home/rubiales/PycharmProjects/pycharm/NLP/ignore_files/w2v_padaseq.csv')\n",
    "X_pad_test.drop(columns=['Unnamed: 0'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre_trained glove2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-04T05:28:07.290788Z",
     "start_time": "2019-12-04T05:27:56.945733Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('/home/alberto/Escritorio/pycharm/NLP/ignore_files/glove.840B.300d.txt') as f:\n",
    "    content = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-04T05:32:01.663807Z",
     "start_time": "2019-12-04T05:28:17.798248Z"
    }
   },
   "outputs": [],
   "source": [
    "embeddings_index = {}\n",
    "\n",
    "for line in tqdm.tqdm_notebook(content):\n",
    "    values = line.split(' ')\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-04T05:37:24.942200Z",
     "start_time": "2019-12-04T05:37:24.937864Z"
    }
   },
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = 150\n",
    "EMBEDDING_DIM = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-04T05:37:28.574615Z",
     "start_time": "2019-12-04T05:37:28.311202Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((len(fourgram_model.wv.vocab) + 1, EMBEDDING_DIM))\n",
    "suma = 0\n",
    "for word, i in tqdm.notebook.tqdm(zip(fourgram_model.wv.vocab.keys(), range(len(fourgram_model.wv.vocab.keys())))):\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i-suma] = embedding_vector\n",
    "    else:\n",
    "        suma += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_tensorboard = 'ignore_files/logs'\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=path_tensorboard, write_graph=True, write_images=True)\n",
    "\n",
    "path_checkpoint = 'ignore_files/model_checkpoint/'\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(path_checkpoint, monitor='accuracy', save_best_only=True,\n",
    "                                                         save_weights_only=True, mode='max')\n",
    "\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(monitor='accuracy', patience=50, mode='max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1')\n",
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, 150, 256)          32472064  \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 150, 128)          98432     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1 (None, 50, 128)           0         \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 50, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_6 (Conv1D)            (None, 50, 64)            24640     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1 (None, 16, 64)            0         \n",
      "_________________________________________________________________\n",
      "bidirectional_5 (Bidirection (None, 256)               197632    \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 120)               30840     \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 1)                 121       \n",
      "=================================================================\n",
      "Total params: 32,823,729\n",
      "Trainable params: 351,665\n",
      "Non-trainable params: 32,472,064\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "mirrored_strategy = tf.distribute.MirroredStrategy()\n",
    "with mirrored_strategy.scope():\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim = fourgram_model.wv.vectors.shape[0],\n",
    "                       output_dim = fourgram_model.wv.vectors.shape[1],\n",
    "                       input_length = input_length,\n",
    "                       weights = [fourgram_model.wv.vectors],\n",
    "                       trainable = False))\n",
    "\n",
    "    model.add(Conv1D(128, 3,\n",
    "                     activation='relu', padding='same'))\n",
    "    model.add(MaxPool1D(pool_size=3))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Conv1D(64, 3,\n",
    "                     activation='relu', padding='same'))\n",
    "    model.add(MaxPool1D(pool_size=3))\n",
    "    model.add(Bidirectional(LSTM(128, recurrent_dropout=0.1)))\n",
    "    model.add(Dropout(0.20))\n",
    "    model.add(Dense(120))\n",
    "#     model.add(Dropout(0.25))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-14T19:29:37.732252Z",
     "start_time": "2019-12-14T19:28:06.874095Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 75000 samples\n",
      "Epoch 1/2000\n",
      "INFO:tensorflow:batch_all_reduce: 14 all-reduces with algorithm = nccl, num_packs = 1, agg_small_grads_max_bytes = 0 and agg_small_grads_max_group = 10\n",
      "INFO:tensorflow:batch_all_reduce: 14 all-reduces with algorithm = nccl, num_packs = 1, agg_small_grads_max_bytes = 0 and agg_small_grads_max_group = 10\n",
      "75000/75000 [==============================] - 10s 135us/sample - loss: 0.7644 - accuracy: 0.5012\n",
      "Epoch 2/2000\n",
      "75000/75000 [==============================] - 2s 24us/sample - loss: 0.6949 - accuracy: 0.5142\n",
      "Epoch 3/2000\n",
      "75000/75000 [==============================] - 2s 24us/sample - loss: 0.6813 - accuracy: 0.5660\n",
      "Epoch 4/2000\n",
      "75000/75000 [==============================] - 2s 24us/sample - loss: 0.6443 - accuracy: 0.6291\n",
      "Epoch 5/2000\n",
      "75000/75000 [==============================] - 2s 24us/sample - loss: 0.5977 - accuracy: 0.6779\n",
      "Epoch 6/2000\n",
      "75000/75000 [==============================] - 2s 23us/sample - loss: 0.5616 - accuracy: 0.7104\n",
      "Epoch 7/2000\n",
      "75000/75000 [==============================] - 2s 24us/sample - loss: 0.5349 - accuracy: 0.7324\n",
      "Epoch 8/2000\n",
      "75000/75000 [==============================] - 2s 24us/sample - loss: 0.5184 - accuracy: 0.7434\n",
      "Epoch 9/2000\n",
      "75000/75000 [==============================] - 2s 24us/sample - loss: 0.4965 - accuracy: 0.7586\n",
      "Epoch 10/2000\n",
      "75000/75000 [==============================] - 2s 24us/sample - loss: 0.4850 - accuracy: 0.7655\n",
      "Epoch 11/2000\n",
      "75000/75000 [==============================] - 2s 24us/sample - loss: 0.4765 - accuracy: 0.7718\n",
      "Epoch 12/2000\n",
      "75000/75000 [==============================] - 2s 24us/sample - loss: 0.4586 - accuracy: 0.7841\n",
      "Epoch 13/2000\n",
      "75000/75000 [==============================] - 2s 24us/sample - loss: 0.4602 - accuracy: 0.7815\n",
      "Epoch 14/2000\n",
      "75000/75000 [==============================] - 2s 24us/sample - loss: 0.4384 - accuracy: 0.7956\n",
      "Epoch 15/2000\n",
      "75000/75000 [==============================] - 2s 24us/sample - loss: 0.4325 - accuracy: 0.7998\n",
      "Epoch 16/2000\n",
      "75000/75000 [==============================] - 2s 24us/sample - loss: 0.4304 - accuracy: 0.8009\n",
      "Epoch 17/2000\n",
      "75000/75000 [==============================] - 2s 24us/sample - loss: 0.4117 - accuracy: 0.8131\n",
      "Epoch 18/2000\n",
      "75000/75000 [==============================] - 2s 24us/sample - loss: 0.4147 - accuracy: 0.8101\n",
      "Epoch 19/2000\n",
      "75000/75000 [==============================] - 2s 24us/sample - loss: 0.4066 - accuracy: 0.8160\n",
      "Epoch 20/2000\n",
      "75000/75000 [==============================] - 2s 24us/sample - loss: 0.3998 - accuracy: 0.8181\n",
      "Epoch 21/2000\n",
      "75000/75000 [==============================] - 2s 24us/sample - loss: 0.3908 - accuracy: 0.8234\n",
      "Epoch 22/2000\n",
      "75000/75000 [==============================] - 2s 24us/sample - loss: 0.3782 - accuracy: 0.8307\n",
      "Epoch 23/2000\n",
      "75000/75000 [==============================] - 2s 24us/sample - loss: 0.3798 - accuracy: 0.8299\n",
      "Epoch 24/2000\n",
      "75000/75000 [==============================] - 2s 24us/sample - loss: 0.4047 - accuracy: 0.8156\n",
      "Epoch 25/2000\n",
      "75000/75000 [==============================] - 2s 24us/sample - loss: 0.3961 - accuracy: 0.8203\n",
      "Epoch 26/2000\n",
      "75000/75000 [==============================] - 2s 24us/sample - loss: 0.3714 - accuracy: 0.8346\n",
      "Epoch 27/2000\n",
      "75000/75000 [==============================] - 2s 24us/sample - loss: 0.3581 - accuracy: 0.8421\n",
      "Epoch 28/2000\n",
      "75000/75000 [==============================] - 2s 24us/sample - loss: 0.3474 - accuracy: 0.8469\n",
      "Epoch 29/2000\n",
      "75000/75000 [==============================] - 2s 24us/sample - loss: 0.3582 - accuracy: 0.8420\n",
      "Epoch 30/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.3571 - accuracy: 0.8422\n",
      "Epoch 31/2000\n",
      "75000/75000 [==============================] - 2s 24us/sample - loss: 0.3518 - accuracy: 0.8451\n",
      "Epoch 32/2000\n",
      "75000/75000 [==============================] - 2s 24us/sample - loss: 0.3302 - accuracy: 0.8579\n",
      "Epoch 33/2000\n",
      "75000/75000 [==============================] - 2s 24us/sample - loss: 0.3533 - accuracy: 0.8443\n",
      "Epoch 34/2000\n",
      "75000/75000 [==============================] - 2s 24us/sample - loss: 0.3323 - accuracy: 0.8549\n",
      "Epoch 35/2000\n",
      "75000/75000 [==============================] - 2s 24us/sample - loss: 0.3338 - accuracy: 0.8540\n",
      "Epoch 36/2000\n",
      "75000/75000 [==============================] - 2s 24us/sample - loss: 0.3367 - accuracy: 0.8518\n",
      "Epoch 37/2000\n",
      "75000/75000 [==============================] - 2s 24us/sample - loss: 0.3200 - accuracy: 0.8610\n",
      "Epoch 38/2000\n",
      "75000/75000 [==============================] - 2s 24us/sample - loss: 0.3144 - accuracy: 0.8654\n",
      "Epoch 39/2000\n",
      "75000/75000 [==============================] - 2s 24us/sample - loss: 0.3183 - accuracy: 0.8615\n",
      "Epoch 40/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.3200 - accuracy: 0.8601\n",
      "Epoch 41/2000\n",
      "75000/75000 [==============================] - 2s 24us/sample - loss: 0.3121 - accuracy: 0.8653\n",
      "Epoch 42/2000\n",
      "75000/75000 [==============================] - 2s 24us/sample - loss: 0.2984 - accuracy: 0.8733\n",
      "Epoch 43/2000\n",
      "75000/75000 [==============================] - 2s 24us/sample - loss: 0.3039 - accuracy: 0.8694\n",
      "Epoch 44/2000\n",
      "75000/75000 [==============================] - 2s 24us/sample - loss: 0.2993 - accuracy: 0.8711\n",
      "Epoch 45/2000\n",
      "75000/75000 [==============================] - 2s 24us/sample - loss: 0.2959 - accuracy: 0.8740\n",
      "Epoch 46/2000\n",
      "75000/75000 [==============================] - 2s 24us/sample - loss: 0.2995 - accuracy: 0.8723\n",
      "Epoch 47/2000\n",
      "75000/75000 [==============================] - 2s 24us/sample - loss: 0.2884 - accuracy: 0.8788\n",
      "Epoch 48/2000\n",
      "75000/75000 [==============================] - 2s 24us/sample - loss: 0.2791 - accuracy: 0.8821\n",
      "Epoch 49/2000\n",
      "75000/75000 [==============================] - 2s 24us/sample - loss: 0.3124 - accuracy: 0.8648\n",
      "Epoch 50/2000\n",
      "75000/75000 [==============================] - 2s 24us/sample - loss: 0.2923 - accuracy: 0.8755\n",
      "Epoch 51/2000\n",
      "75000/75000 [==============================] - 2s 24us/sample - loss: 0.2796 - accuracy: 0.8819\n",
      "Epoch 52/2000\n",
      "75000/75000 [==============================] - 2s 24us/sample - loss: 0.2652 - accuracy: 0.8893\n",
      "Epoch 53/2000\n",
      "75000/75000 [==============================] - 2s 24us/sample - loss: 0.2919 - accuracy: 0.8756\n",
      "Epoch 54/2000\n",
      "75000/75000 [==============================] - 2s 24us/sample - loss: 0.2667 - accuracy: 0.8883\n",
      "Epoch 55/2000\n",
      "75000/75000 [==============================] - 2s 24us/sample - loss: 0.2592 - accuracy: 0.8920\n",
      "Epoch 56/2000\n",
      "75000/75000 [==============================] - 2s 24us/sample - loss: 0.2804 - accuracy: 0.8805\n",
      "Epoch 57/2000\n",
      "75000/75000 [==============================] - 2s 24us/sample - loss: 0.2592 - accuracy: 0.8925\n",
      "Epoch 58/2000\n",
      "75000/75000 [==============================] - 2s 24us/sample - loss: 0.2530 - accuracy: 0.8949\n",
      "Epoch 59/2000\n",
      "75000/75000 [==============================] - 2s 24us/sample - loss: 0.2584 - accuracy: 0.8915\n",
      "Epoch 60/2000\n",
      "75000/75000 [==============================] - 2s 24us/sample - loss: 0.2547 - accuracy: 0.8939\n",
      "Epoch 61/2000\n",
      "75000/75000 [==============================] - 2s 24us/sample - loss: 0.2459 - accuracy: 0.8965\n",
      "Epoch 62/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.2678 - accuracy: 0.8874\n",
      "Epoch 63/2000\n",
      "75000/75000 [==============================] - 2s 24us/sample - loss: 0.2536 - accuracy: 0.8952\n",
      "Epoch 64/2000\n",
      "75000/75000 [==============================] - 2s 24us/sample - loss: 0.2368 - accuracy: 0.9019\n",
      "Epoch 65/2000\n",
      "75000/75000 [==============================] - 2s 24us/sample - loss: 0.2398 - accuracy: 0.9010\n",
      "Epoch 66/2000\n",
      "75000/75000 [==============================] - 2s 24us/sample - loss: 0.2446 - accuracy: 0.8976\n",
      "Epoch 67/2000\n",
      "75000/75000 [==============================] - 2s 24us/sample - loss: 0.2273 - accuracy: 0.9068\n",
      "Epoch 68/2000\n",
      "75000/75000 [==============================] - 2s 24us/sample - loss: 0.2250 - accuracy: 0.9080\n",
      "Epoch 69/2000\n",
      "75000/75000 [==============================] - 2s 24us/sample - loss: 0.2432 - accuracy: 0.8989\n",
      "Epoch 70/2000\n",
      "75000/75000 [==============================] - 2s 24us/sample - loss: 0.2374 - accuracy: 0.9029\n",
      "Epoch 71/2000\n",
      "75000/75000 [==============================] - 2s 24us/sample - loss: 0.2483 - accuracy: 0.8976\n",
      "Epoch 72/2000\n",
      "75000/75000 [==============================] - 2s 24us/sample - loss: 0.2160 - accuracy: 0.9124\n",
      "Epoch 73/2000\n",
      "75000/75000 [==============================] - 2s 24us/sample - loss: 0.2134 - accuracy: 0.9136\n",
      "Epoch 74/2000\n",
      "75000/75000 [==============================] - 2s 24us/sample - loss: 0.2105 - accuracy: 0.9149\n",
      "Epoch 75/2000\n",
      "75000/75000 [==============================] - 2s 24us/sample - loss: 0.2201 - accuracy: 0.9094\n",
      "Epoch 76/2000\n",
      "75000/75000 [==============================] - 2s 24us/sample - loss: 0.2097 - accuracy: 0.9144\n",
      "Epoch 77/2000\n",
      "75000/75000 [==============================] - 2s 24us/sample - loss: 0.2222 - accuracy: 0.9076\n",
      "Epoch 78/2000\n",
      "75000/75000 [==============================] - 2s 24us/sample - loss: 0.2181 - accuracy: 0.9096\n",
      "Epoch 79/2000\n",
      "75000/75000 [==============================] - 2s 24us/sample - loss: 0.2025 - accuracy: 0.9170\n",
      "Epoch 80/2000\n",
      "75000/75000 [==============================] - 2s 24us/sample - loss: 0.2017 - accuracy: 0.9176\n",
      "Epoch 81/2000\n",
      "75000/75000 [==============================] - 2s 24us/sample - loss: 0.2216 - accuracy: 0.9075\n",
      "Epoch 82/2000\n",
      "75000/75000 [==============================] - 2s 24us/sample - loss: 0.2039 - accuracy: 0.9165\n",
      "Epoch 83/2000\n",
      "75000/75000 [==============================] - 2s 24us/sample - loss: 0.1983 - accuracy: 0.9192\n",
      "Epoch 84/2000\n",
      "75000/75000 [==============================] - 2s 24us/sample - loss: 0.1941 - accuracy: 0.9213\n",
      "Epoch 85/2000\n",
      "75000/75000 [==============================] - 2s 24us/sample - loss: 0.1980 - accuracy: 0.9191\n",
      "Epoch 86/2000\n",
      "75000/75000 [==============================] - 2s 24us/sample - loss: 0.1942 - accuracy: 0.9214\n",
      "Epoch 87/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.1849 - accuracy: 0.9256\n",
      "Epoch 88/2000\n",
      "75000/75000 [==============================] - 2s 24us/sample - loss: 0.1889 - accuracy: 0.9223\n",
      "Epoch 89/2000\n",
      "75000/75000 [==============================] - 2s 24us/sample - loss: 0.1886 - accuracy: 0.9237\n",
      "Epoch 90/2000\n",
      "75000/75000 [==============================] - 2s 24us/sample - loss: 0.1890 - accuracy: 0.9233\n",
      "Epoch 91/2000\n",
      "75000/75000 [==============================] - 2s 24us/sample - loss: 0.1831 - accuracy: 0.9257\n",
      "Epoch 92/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.1983 - accuracy: 0.9179\n",
      "Epoch 93/2000\n",
      "75000/75000 [==============================] - 2s 24us/sample - loss: 0.1837 - accuracy: 0.9253\n",
      "Epoch 94/2000\n",
      "75000/75000 [==============================] - 2s 24us/sample - loss: 0.1798 - accuracy: 0.9271\n",
      "Epoch 95/2000\n",
      "75000/75000 [==============================] - 2s 24us/sample - loss: 0.1823 - accuracy: 0.9265\n",
      "Epoch 96/2000\n",
      "75000/75000 [==============================] - 2s 24us/sample - loss: 0.1735 - accuracy: 0.9295\n",
      "Epoch 97/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.1872 - accuracy: 0.9232\n",
      "Epoch 98/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.1779 - accuracy: 0.9282\n",
      "Epoch 99/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.1661 - accuracy: 0.9334\n",
      "Epoch 100/2000\n",
      "75000/75000 [==============================] - 2s 24us/sample - loss: 0.1728 - accuracy: 0.9296\n",
      "Epoch 101/2000\n",
      "75000/75000 [==============================] - 2s 24us/sample - loss: 0.1812 - accuracy: 0.9264\n",
      "Epoch 102/2000\n",
      "75000/75000 [==============================] - 2s 24us/sample - loss: 0.1831 - accuracy: 0.9263\n",
      "Epoch 103/2000\n",
      "75000/75000 [==============================] - 2s 24us/sample - loss: 0.1633 - accuracy: 0.9347\n",
      "Epoch 104/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.1680 - accuracy: 0.9318\n",
      "Epoch 105/2000\n",
      "75000/75000 [==============================] - 2s 24us/sample - loss: 0.1684 - accuracy: 0.9325\n",
      "Epoch 106/2000\n",
      "75000/75000 [==============================] - 2s 24us/sample - loss: 0.1666 - accuracy: 0.9329\n",
      "Epoch 107/2000\n",
      "75000/75000 [==============================] - 2s 24us/sample - loss: 0.1565 - accuracy: 0.9374\n",
      "Epoch 108/2000\n",
      "75000/75000 [==============================] - 2s 24us/sample - loss: 0.1589 - accuracy: 0.9362\n",
      "Epoch 109/2000\n",
      "75000/75000 [==============================] - 2s 24us/sample - loss: 0.1547 - accuracy: 0.9377\n",
      "Epoch 110/2000\n",
      "75000/75000 [==============================] - 2s 24us/sample - loss: 0.1546 - accuracy: 0.9383\n",
      "Epoch 111/2000\n",
      "75000/75000 [==============================] - 2s 24us/sample - loss: 0.1724 - accuracy: 0.9302\n",
      "Epoch 112/2000\n",
      "75000/75000 [==============================] - 2s 24us/sample - loss: 0.1582 - accuracy: 0.9369\n",
      "Epoch 113/2000\n",
      "75000/75000 [==============================] - 2s 24us/sample - loss: 0.1524 - accuracy: 0.9389\n",
      "Epoch 114/2000\n",
      "75000/75000 [==============================] - 2s 24us/sample - loss: 0.1539 - accuracy: 0.9388\n",
      "Epoch 115/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.1561 - accuracy: 0.9375\n",
      "Epoch 116/2000\n",
      "75000/75000 [==============================] - 2s 24us/sample - loss: 0.1536 - accuracy: 0.9390\n",
      "Epoch 117/2000\n",
      "75000/75000 [==============================] - 2s 24us/sample - loss: 0.1463 - accuracy: 0.9423\n",
      "Epoch 118/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.1421 - accuracy: 0.9437\n",
      "Epoch 119/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.1597 - accuracy: 0.9359\n",
      "Epoch 120/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.1505 - accuracy: 0.9399\n",
      "Epoch 121/2000\n",
      "75000/75000 [==============================] - 2s 24us/sample - loss: 0.1409 - accuracy: 0.9432\n",
      "Epoch 122/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.1597 - accuracy: 0.9352\n",
      "Epoch 123/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.1467 - accuracy: 0.9416\n",
      "Epoch 124/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.1536 - accuracy: 0.9393\n",
      "Epoch 125/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.1650 - accuracy: 0.9332\n",
      "Epoch 126/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.1588 - accuracy: 0.9367\n",
      "Epoch 127/2000\n",
      "75000/75000 [==============================] - 2s 26us/sample - loss: 0.1384 - accuracy: 0.9458\n",
      "Epoch 128/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.1332 - accuracy: 0.9479\n",
      "Epoch 129/2000\n",
      "75000/75000 [==============================] - 2s 24us/sample - loss: 0.1385 - accuracy: 0.9449\n",
      "Epoch 130/2000\n",
      "75000/75000 [==============================] - 2s 24us/sample - loss: 0.1473 - accuracy: 0.9415\n",
      "Epoch 131/2000\n",
      "75000/75000 [==============================] - 2s 24us/sample - loss: 0.1435 - accuracy: 0.9424\n",
      "Epoch 132/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.1469 - accuracy: 0.9415\n",
      "Epoch 133/2000\n",
      "75000/75000 [==============================] - 2s 24us/sample - loss: 0.1360 - accuracy: 0.9463\n",
      "Epoch 134/2000\n",
      "75000/75000 [==============================] - 2s 24us/sample - loss: 0.1337 - accuracy: 0.9470\n",
      "Epoch 135/2000\n",
      "75000/75000 [==============================] - 2s 24us/sample - loss: 0.1394 - accuracy: 0.9446\n",
      "Epoch 136/2000\n",
      "75000/75000 [==============================] - 2s 24us/sample - loss: 0.1375 - accuracy: 0.9454\n",
      "Epoch 137/2000\n",
      "75000/75000 [==============================] - 2s 24us/sample - loss: 0.1290 - accuracy: 0.9492\n",
      "Epoch 138/2000\n",
      "75000/75000 [==============================] - 2s 24us/sample - loss: 0.1330 - accuracy: 0.9472\n",
      "Epoch 139/2000\n",
      "75000/75000 [==============================] - 2s 24us/sample - loss: 0.1334 - accuracy: 0.9467\n",
      "Epoch 140/2000\n",
      "75000/75000 [==============================] - 2s 24us/sample - loss: 0.1662 - accuracy: 0.9335\n",
      "Epoch 141/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.1398 - accuracy: 0.9450\n",
      "Epoch 142/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.1315 - accuracy: 0.9486\n",
      "Epoch 143/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.1333 - accuracy: 0.9470\n",
      "Epoch 144/2000\n",
      "75000/75000 [==============================] - 2s 24us/sample - loss: 0.1220 - accuracy: 0.9520\n",
      "Epoch 145/2000\n",
      "75000/75000 [==============================] - 2s 24us/sample - loss: 0.1218 - accuracy: 0.9522\n",
      "Epoch 146/2000\n",
      "75000/75000 [==============================] - 2s 24us/sample - loss: 0.1265 - accuracy: 0.9499\n",
      "Epoch 147/2000\n",
      "75000/75000 [==============================] - 2s 24us/sample - loss: 0.1195 - accuracy: 0.9524\n",
      "Epoch 148/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.1179 - accuracy: 0.9539\n",
      "Epoch 149/2000\n",
      "75000/75000 [==============================] - 2s 24us/sample - loss: 0.1303 - accuracy: 0.9484\n",
      "Epoch 150/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.1205 - accuracy: 0.9522\n",
      "Epoch 151/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.1180 - accuracy: 0.9531\n",
      "Epoch 152/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.1212 - accuracy: 0.9517\n",
      "Epoch 153/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.1178 - accuracy: 0.9545\n",
      "Epoch 154/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.1292 - accuracy: 0.9486\n",
      "Epoch 155/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.1184 - accuracy: 0.9531\n",
      "Epoch 156/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.1222 - accuracy: 0.9521\n",
      "Epoch 157/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.1198 - accuracy: 0.9529\n",
      "Epoch 158/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.1201 - accuracy: 0.9526\n",
      "Epoch 159/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.1161 - accuracy: 0.9543\n",
      "Epoch 160/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.1146 - accuracy: 0.9546\n",
      "Epoch 161/2000\n",
      "75000/75000 [==============================] - 2s 24us/sample - loss: 0.1240 - accuracy: 0.9507\n",
      "Epoch 162/2000\n",
      "75000/75000 [==============================] - 2s 24us/sample - loss: 0.1161 - accuracy: 0.9538\n",
      "Epoch 163/2000\n",
      "75000/75000 [==============================] - 2s 24us/sample - loss: 0.1083 - accuracy: 0.9576\n",
      "Epoch 164/2000\n",
      "75000/75000 [==============================] - 2s 24us/sample - loss: 0.1095 - accuracy: 0.9564\n",
      "Epoch 165/2000\n",
      "75000/75000 [==============================] - 2s 24us/sample - loss: 0.1107 - accuracy: 0.9572\n",
      "Epoch 166/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.1139 - accuracy: 0.9546\n",
      "Epoch 167/2000\n",
      "75000/75000 [==============================] - 2s 24us/sample - loss: 0.1186 - accuracy: 0.9529\n",
      "Epoch 168/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.1099 - accuracy: 0.9571\n",
      "Epoch 169/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.1065 - accuracy: 0.9581\n",
      "Epoch 170/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.1026 - accuracy: 0.9596\n",
      "Epoch 171/2000\n",
      "75000/75000 [==============================] - 2s 24us/sample - loss: 0.1047 - accuracy: 0.9593\n",
      "Epoch 172/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.1093 - accuracy: 0.9575\n",
      "Epoch 173/2000\n",
      "75000/75000 [==============================] - 2s 24us/sample - loss: 0.1151 - accuracy: 0.9547\n",
      "Epoch 174/2000\n",
      "75000/75000 [==============================] - 2s 24us/sample - loss: 0.1144 - accuracy: 0.9547\n",
      "Epoch 175/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.1045 - accuracy: 0.9589\n",
      "Epoch 176/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.1048 - accuracy: 0.9591\n",
      "Epoch 177/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.1014 - accuracy: 0.9608\n",
      "Epoch 178/2000\n",
      "75000/75000 [==============================] - 2s 24us/sample - loss: 0.1050 - accuracy: 0.9595\n",
      "Epoch 179/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.1034 - accuracy: 0.9598\n",
      "Epoch 180/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.1110 - accuracy: 0.9569\n",
      "Epoch 181/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.1141 - accuracy: 0.9551\n",
      "Epoch 182/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.1020 - accuracy: 0.9601\n",
      "Epoch 183/2000\n",
      "75000/75000 [==============================] - 2s 24us/sample - loss: 0.1222 - accuracy: 0.9511\n",
      "Epoch 184/2000\n",
      "75000/75000 [==============================] - 2s 24us/sample - loss: 0.1075 - accuracy: 0.9575\n",
      "Epoch 185/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.1016 - accuracy: 0.9600\n",
      "Epoch 186/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.1115 - accuracy: 0.9565\n",
      "Epoch 187/2000\n",
      "75000/75000 [==============================] - 2s 24us/sample - loss: 0.1101 - accuracy: 0.9562\n",
      "Epoch 188/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.0972 - accuracy: 0.9622\n",
      "Epoch 189/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.1059 - accuracy: 0.9585\n",
      "Epoch 190/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.1070 - accuracy: 0.9584\n",
      "Epoch 191/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.0989 - accuracy: 0.9614\n",
      "Epoch 192/2000\n",
      "75000/75000 [==============================] - 2s 24us/sample - loss: 0.0968 - accuracy: 0.9623\n",
      "Epoch 193/2000\n",
      "75000/75000 [==============================] - 2s 24us/sample - loss: 0.0924 - accuracy: 0.9644\n",
      "Epoch 194/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.0946 - accuracy: 0.9635\n",
      "Epoch 195/2000\n",
      "75000/75000 [==============================] - 2s 24us/sample - loss: 0.1227 - accuracy: 0.9517\n",
      "Epoch 196/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.1307 - accuracy: 0.9475\n",
      "Epoch 197/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.1071 - accuracy: 0.9574\n",
      "Epoch 198/2000\n",
      "75000/75000 [==============================] - 2s 24us/sample - loss: 0.0969 - accuracy: 0.9624\n",
      "Epoch 199/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.0955 - accuracy: 0.9631\n",
      "Epoch 200/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.1002 - accuracy: 0.9606\n",
      "Epoch 201/2000\n",
      "75000/75000 [==============================] - 2s 24us/sample - loss: 0.0932 - accuracy: 0.9635\n",
      "Epoch 202/2000\n",
      "75000/75000 [==============================] - 2s 24us/sample - loss: 0.0968 - accuracy: 0.9627\n",
      "Epoch 203/2000\n",
      "75000/75000 [==============================] - 2s 24us/sample - loss: 0.0935 - accuracy: 0.9632\n",
      "Epoch 204/2000\n",
      "75000/75000 [==============================] - 2s 24us/sample - loss: 0.0962 - accuracy: 0.9618\n",
      "Epoch 205/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.0911 - accuracy: 0.9647\n",
      "Epoch 206/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.0903 - accuracy: 0.9648\n",
      "Epoch 207/2000\n",
      "75000/75000 [==============================] - 2s 26us/sample - loss: 0.0902 - accuracy: 0.9651\n",
      "Epoch 208/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.0940 - accuracy: 0.9642\n",
      "Epoch 209/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.1071 - accuracy: 0.9578\n",
      "Epoch 210/2000\n",
      "75000/75000 [==============================] - 2s 24us/sample - loss: 0.0902 - accuracy: 0.9654\n",
      "Epoch 211/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.0844 - accuracy: 0.9678\n",
      "Epoch 212/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.0860 - accuracy: 0.9667\n",
      "Epoch 213/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.0932 - accuracy: 0.9643\n",
      "Epoch 214/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.1134 - accuracy: 0.9549\n",
      "Epoch 215/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.0977 - accuracy: 0.9617\n",
      "Epoch 216/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.0856 - accuracy: 0.9671\n",
      "Epoch 217/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.0878 - accuracy: 0.9654\n",
      "Epoch 218/2000\n",
      "75000/75000 [==============================] - 2s 24us/sample - loss: 0.0894 - accuracy: 0.9661\n",
      "Epoch 219/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.0930 - accuracy: 0.9640\n",
      "Epoch 220/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.0888 - accuracy: 0.9655\n",
      "Epoch 221/2000\n",
      "75000/75000 [==============================] - 2s 26us/sample - loss: 0.0853 - accuracy: 0.9674\n",
      "Epoch 222/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.0891 - accuracy: 0.9657\n",
      "Epoch 223/2000\n",
      "75000/75000 [==============================] - 2s 24us/sample - loss: 0.0929 - accuracy: 0.9637\n",
      "Epoch 224/2000\n",
      "75000/75000 [==============================] - 2s 24us/sample - loss: 0.0899 - accuracy: 0.9652\n",
      "Epoch 225/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.0841 - accuracy: 0.9679\n",
      "Epoch 226/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.0863 - accuracy: 0.9668\n",
      "Epoch 227/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.0870 - accuracy: 0.9661\n",
      "Epoch 228/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.0866 - accuracy: 0.9666\n",
      "Epoch 229/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.0822 - accuracy: 0.9678\n",
      "Epoch 230/2000\n",
      "75000/75000 [==============================] - 2s 24us/sample - loss: 0.0853 - accuracy: 0.9670\n",
      "Epoch 231/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.0999 - accuracy: 0.9609\n",
      "Epoch 232/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.0892 - accuracy: 0.9656\n",
      "Epoch 233/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.0912 - accuracy: 0.9644\n",
      "Epoch 234/2000\n",
      "75000/75000 [==============================] - 2s 26us/sample - loss: 0.0979 - accuracy: 0.9620\n",
      "Epoch 235/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.0903 - accuracy: 0.9656\n",
      "Epoch 236/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.0808 - accuracy: 0.9687\n",
      "Epoch 237/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.0821 - accuracy: 0.9678\n",
      "Epoch 238/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.0928 - accuracy: 0.9643\n",
      "Epoch 239/2000\n",
      "75000/75000 [==============================] - 2s 24us/sample - loss: 0.0873 - accuracy: 0.9666\n",
      "Epoch 240/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.0902 - accuracy: 0.9654\n",
      "Epoch 241/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.0800 - accuracy: 0.9689\n",
      "Epoch 242/2000\n",
      "75000/75000 [==============================] - 2s 26us/sample - loss: 0.0817 - accuracy: 0.9686\n",
      "Epoch 243/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.0779 - accuracy: 0.9700\n",
      "Epoch 244/2000\n",
      "75000/75000 [==============================] - 2s 24us/sample - loss: 0.0755 - accuracy: 0.9713\n",
      "Epoch 245/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.0772 - accuracy: 0.9705\n",
      "Epoch 246/2000\n",
      "75000/75000 [==============================] - 2s 24us/sample - loss: 0.0761 - accuracy: 0.9701\n",
      "Epoch 247/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.0787 - accuracy: 0.9700\n",
      "Epoch 248/2000\n",
      "75000/75000 [==============================] - 2s 24us/sample - loss: 0.0785 - accuracy: 0.9695\n",
      "Epoch 249/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.0788 - accuracy: 0.9698\n",
      "Epoch 250/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.0888 - accuracy: 0.9658\n",
      "Epoch 251/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.0902 - accuracy: 0.9656\n",
      "Epoch 252/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.0766 - accuracy: 0.9703\n",
      "Epoch 253/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.0846 - accuracy: 0.9670\n",
      "Epoch 254/2000\n",
      "75000/75000 [==============================] - 2s 24us/sample - loss: 0.0851 - accuracy: 0.9670\n",
      "Epoch 255/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.0811 - accuracy: 0.9684\n",
      "Epoch 256/2000\n",
      "75000/75000 [==============================] - 2s 24us/sample - loss: 0.0792 - accuracy: 0.9693\n",
      "Epoch 257/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.0750 - accuracy: 0.9717\n",
      "Epoch 258/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.0732 - accuracy: 0.9724\n",
      "Epoch 259/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.0815 - accuracy: 0.9686\n",
      "Epoch 260/2000\n",
      "75000/75000 [==============================] - 2s 24us/sample - loss: 0.0759 - accuracy: 0.9707\n",
      "Epoch 261/2000\n",
      "75000/75000 [==============================] - 2s 24us/sample - loss: 0.0799 - accuracy: 0.9686\n",
      "Epoch 262/2000\n",
      "75000/75000 [==============================] - 2s 24us/sample - loss: 0.0723 - accuracy: 0.9725\n",
      "Epoch 263/2000\n",
      "75000/75000 [==============================] - 2s 24us/sample - loss: 0.0777 - accuracy: 0.9700\n",
      "Epoch 264/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.1971 - accuracy: 0.9208\n",
      "Epoch 265/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.1266 - accuracy: 0.9489\n",
      "Epoch 266/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.0960 - accuracy: 0.9623\n",
      "Epoch 267/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.0754 - accuracy: 0.9712\n",
      "Epoch 268/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.0723 - accuracy: 0.9722\n",
      "Epoch 269/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.0734 - accuracy: 0.9719\n",
      "Epoch 270/2000\n",
      "75000/75000 [==============================] - 2s 24us/sample - loss: 0.0739 - accuracy: 0.9718\n",
      "Epoch 271/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.0767 - accuracy: 0.9704\n",
      "Epoch 272/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.0803 - accuracy: 0.9692\n",
      "Epoch 273/2000\n",
      "75000/75000 [==============================] - 2s 24us/sample - loss: 0.0707 - accuracy: 0.9737\n",
      "Epoch 274/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.0717 - accuracy: 0.9728\n",
      "Epoch 275/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.0704 - accuracy: 0.9729\n",
      "Epoch 276/2000\n",
      "75000/75000 [==============================] - 2s 26us/sample - loss: 0.0693 - accuracy: 0.9735\n",
      "Epoch 277/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.0686 - accuracy: 0.9739\n",
      "Epoch 278/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.0705 - accuracy: 0.9729\n",
      "Epoch 279/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.0707 - accuracy: 0.9725\n",
      "Epoch 280/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.0695 - accuracy: 0.9734\n",
      "Epoch 281/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.0714 - accuracy: 0.9725\n",
      "Epoch 282/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.0694 - accuracy: 0.9740\n",
      "Epoch 283/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.0704 - accuracy: 0.9731\n",
      "Epoch 284/2000\n",
      "75000/75000 [==============================] - 2s 27us/sample - loss: 0.0700 - accuracy: 0.9732\n",
      "Epoch 285/2000\n",
      "75000/75000 [==============================] - 2s 24us/sample - loss: 0.0648 - accuracy: 0.9749\n",
      "Epoch 286/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.0778 - accuracy: 0.9705\n",
      "Epoch 287/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.0697 - accuracy: 0.9731\n",
      "Epoch 288/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.0696 - accuracy: 0.9731\n",
      "Epoch 289/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.0735 - accuracy: 0.9725\n",
      "Epoch 290/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.0792 - accuracy: 0.9698\n",
      "Epoch 291/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.0797 - accuracy: 0.9695\n",
      "Epoch 292/2000\n",
      "75000/75000 [==============================] - 2s 24us/sample - loss: 0.0655 - accuracy: 0.9751\n",
      "Epoch 293/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.0695 - accuracy: 0.9734\n",
      "Epoch 294/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.0699 - accuracy: 0.9735\n",
      "Epoch 295/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.0650 - accuracy: 0.9752\n",
      "Epoch 296/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.0701 - accuracy: 0.9739\n",
      "Epoch 297/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.0685 - accuracy: 0.9735\n",
      "Epoch 298/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.0743 - accuracy: 0.9712\n",
      "Epoch 299/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.0656 - accuracy: 0.9748\n",
      "Epoch 300/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.0643 - accuracy: 0.9749\n",
      "Epoch 301/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.0653 - accuracy: 0.9750\n",
      "Epoch 302/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.0759 - accuracy: 0.9707\n",
      "Epoch 303/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.0872 - accuracy: 0.9663\n",
      "Epoch 304/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.0849 - accuracy: 0.9666\n",
      "Epoch 305/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.0643 - accuracy: 0.9757\n",
      "Epoch 306/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.0653 - accuracy: 0.9750\n",
      "Epoch 307/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.0659 - accuracy: 0.9749\n",
      "Epoch 308/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.0705 - accuracy: 0.9726\n",
      "Epoch 309/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.0797 - accuracy: 0.9688\n",
      "Epoch 310/2000\n",
      "75000/75000 [==============================] - 2s 26us/sample - loss: 0.0846 - accuracy: 0.9667\n",
      "Epoch 311/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.0761 - accuracy: 0.9707\n",
      "Epoch 312/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.0632 - accuracy: 0.9763\n",
      "Epoch 313/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.0735 - accuracy: 0.9717\n",
      "Epoch 314/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.0675 - accuracy: 0.9745\n",
      "Epoch 315/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.0630 - accuracy: 0.9761\n",
      "Epoch 316/2000\n",
      "75000/75000 [==============================] - 2s 27us/sample - loss: 0.0665 - accuracy: 0.9746\n",
      "Epoch 317/2000\n",
      "75000/75000 [==============================] - 2s 26us/sample - loss: 0.0666 - accuracy: 0.9748\n",
      "Epoch 318/2000\n",
      "75000/75000 [==============================] - 2s 26us/sample - loss: 0.0664 - accuracy: 0.9748\n",
      "Epoch 319/2000\n",
      "75000/75000 [==============================] - 2s 26us/sample - loss: 0.0728 - accuracy: 0.9720\n",
      "Epoch 320/2000\n",
      "75000/75000 [==============================] - 2s 26us/sample - loss: 0.0628 - accuracy: 0.9762\n",
      "Epoch 321/2000\n",
      "75000/75000 [==============================] - 2s 24us/sample - loss: 0.0637 - accuracy: 0.9760\n",
      "Epoch 322/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.0665 - accuracy: 0.9747\n",
      "Epoch 323/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.0620 - accuracy: 0.9767\n",
      "Epoch 324/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.0630 - accuracy: 0.9759\n",
      "Epoch 325/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.0589 - accuracy: 0.9780\n",
      "Epoch 326/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.0610 - accuracy: 0.9770\n",
      "Epoch 327/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.0612 - accuracy: 0.9766\n",
      "Epoch 328/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.0605 - accuracy: 0.9770\n",
      "Epoch 329/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.0682 - accuracy: 0.9745\n",
      "Epoch 330/2000\n",
      "75000/75000 [==============================] - 2s 24us/sample - loss: 0.0632 - accuracy: 0.9766\n",
      "Epoch 331/2000\n",
      "75000/75000 [==============================] - 2s 24us/sample - loss: 0.0644 - accuracy: 0.9758\n",
      "Epoch 332/2000\n",
      "75000/75000 [==============================] - 2s 24us/sample - loss: 0.0613 - accuracy: 0.9768\n",
      "Epoch 333/2000\n",
      "75000/75000 [==============================] - 2s 24us/sample - loss: 0.0595 - accuracy: 0.9772\n",
      "Epoch 334/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.0663 - accuracy: 0.9749\n",
      "Epoch 335/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.0689 - accuracy: 0.9732\n",
      "Epoch 336/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.0598 - accuracy: 0.9775\n",
      "Epoch 337/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.0681 - accuracy: 0.9743\n",
      "Epoch 338/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.0653 - accuracy: 0.9753\n",
      "Epoch 339/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.0607 - accuracy: 0.9772\n",
      "Epoch 340/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.0635 - accuracy: 0.9757\n",
      "Epoch 341/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.0582 - accuracy: 0.9781\n",
      "Epoch 342/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.0685 - accuracy: 0.9740\n",
      "Epoch 343/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.0714 - accuracy: 0.9731\n",
      "Epoch 344/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.0698 - accuracy: 0.9731\n",
      "Epoch 345/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.0607 - accuracy: 0.9770\n",
      "Epoch 346/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.0633 - accuracy: 0.9754\n",
      "Epoch 347/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.0590 - accuracy: 0.9781\n",
      "Epoch 348/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.0581 - accuracy: 0.9781\n",
      "Epoch 349/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.0610 - accuracy: 0.9774\n",
      "Epoch 350/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.0600 - accuracy: 0.9777\n",
      "Epoch 351/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.0571 - accuracy: 0.9788\n",
      "Epoch 352/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.0571 - accuracy: 0.9781\n",
      "Epoch 353/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.0611 - accuracy: 0.9770\n",
      "Epoch 354/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.0606 - accuracy: 0.9770\n",
      "Epoch 355/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.0641 - accuracy: 0.9758\n",
      "Epoch 356/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.0669 - accuracy: 0.9740\n",
      "Epoch 357/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.0708 - accuracy: 0.9729\n",
      "Epoch 358/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.0700 - accuracy: 0.9731\n",
      "Epoch 359/2000\n",
      "75000/75000 [==============================] - 2s 27us/sample - loss: 0.0674 - accuracy: 0.9746\n",
      "Epoch 360/2000\n",
      "75000/75000 [==============================] - 2s 27us/sample - loss: 0.0599 - accuracy: 0.9776\n",
      "Epoch 361/2000\n",
      "75000/75000 [==============================] - 2s 26us/sample - loss: 0.0626 - accuracy: 0.9758\n",
      "Epoch 362/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.0583 - accuracy: 0.9783\n",
      "Epoch 363/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.0588 - accuracy: 0.9782\n",
      "Epoch 364/2000\n",
      "75000/75000 [==============================] - 2s 26us/sample - loss: 0.0616 - accuracy: 0.9763\n",
      "Epoch 365/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.0592 - accuracy: 0.9773\n",
      "Epoch 366/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.0583 - accuracy: 0.9786\n",
      "Epoch 367/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.0563 - accuracy: 0.9785\n",
      "Epoch 368/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.0648 - accuracy: 0.9749\n",
      "Epoch 369/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.0571 - accuracy: 0.9784\n",
      "Epoch 370/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.0552 - accuracy: 0.9793\n",
      "Epoch 371/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.0551 - accuracy: 0.9795\n",
      "Epoch 372/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.0562 - accuracy: 0.9785\n",
      "Epoch 373/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.0546 - accuracy: 0.9795\n",
      "Epoch 374/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.0519 - accuracy: 0.9802\n",
      "Epoch 375/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.0578 - accuracy: 0.9783\n",
      "Epoch 376/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.0727 - accuracy: 0.9721\n",
      "Epoch 377/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.0854 - accuracy: 0.9669\n",
      "Epoch 378/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.0717 - accuracy: 0.9726\n",
      "Epoch 379/2000\n",
      "75000/75000 [==============================] - 2s 27us/sample - loss: 0.0617 - accuracy: 0.9765\n",
      "Epoch 380/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.0668 - accuracy: 0.9744\n",
      "Epoch 381/2000\n",
      "75000/75000 [==============================] - 2s 27us/sample - loss: 0.0575 - accuracy: 0.9781\n",
      "Epoch 382/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.0543 - accuracy: 0.9794\n",
      "Epoch 383/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.0549 - accuracy: 0.9794\n",
      "Epoch 384/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.0577 - accuracy: 0.9785\n",
      "Epoch 385/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.0552 - accuracy: 0.9791\n",
      "Epoch 386/2000\n",
      "75000/75000 [==============================] - 2s 26us/sample - loss: 0.0548 - accuracy: 0.9793\n",
      "Epoch 387/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.0538 - accuracy: 0.9796\n",
      "Epoch 388/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.0694 - accuracy: 0.9731\n",
      "Epoch 389/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.0588 - accuracy: 0.9777\n",
      "Epoch 390/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.0557 - accuracy: 0.9791\n",
      "Epoch 391/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.0595 - accuracy: 0.9780\n",
      "Epoch 392/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.0583 - accuracy: 0.9783\n",
      "Epoch 393/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.0514 - accuracy: 0.9804\n",
      "Epoch 394/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.0610 - accuracy: 0.9770\n",
      "Epoch 395/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.0609 - accuracy: 0.9772\n",
      "Epoch 396/2000\n",
      "75000/75000 [==============================] - 2s 24us/sample - loss: 0.0533 - accuracy: 0.9800\n",
      "Epoch 397/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.0496 - accuracy: 0.9812\n",
      "Epoch 398/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.0552 - accuracy: 0.9797\n",
      "Epoch 399/2000\n",
      "75000/75000 [==============================] - 2s 24us/sample - loss: 0.0519 - accuracy: 0.9807\n",
      "Epoch 400/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.0523 - accuracy: 0.9800\n",
      "Epoch 401/2000\n",
      "75000/75000 [==============================] - 2s 26us/sample - loss: 0.0602 - accuracy: 0.9777\n",
      "Epoch 402/2000\n",
      "75000/75000 [==============================] - 2s 24us/sample - loss: 0.0593 - accuracy: 0.9773\n",
      "Epoch 403/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.0620 - accuracy: 0.9768\n",
      "Epoch 404/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.0594 - accuracy: 0.9776\n",
      "Epoch 405/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.0610 - accuracy: 0.9766\n",
      "Epoch 406/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.0563 - accuracy: 0.9785\n",
      "Epoch 407/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.0528 - accuracy: 0.9803\n",
      "Epoch 408/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.0541 - accuracy: 0.9791\n",
      "Epoch 409/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.0601 - accuracy: 0.9768\n",
      "Epoch 410/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.0585 - accuracy: 0.9781\n",
      "Epoch 411/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.0522 - accuracy: 0.9805\n",
      "Epoch 412/2000\n",
      "75000/75000 [==============================] - 2s 26us/sample - loss: 0.0523 - accuracy: 0.9797\n",
      "Epoch 413/2000\n",
      "75000/75000 [==============================] - 2s 26us/sample - loss: 0.0520 - accuracy: 0.9809\n",
      "Epoch 414/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.0507 - accuracy: 0.9808\n",
      "Epoch 415/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.0558 - accuracy: 0.9791\n",
      "Epoch 416/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.0523 - accuracy: 0.9798\n",
      "Epoch 417/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.0497 - accuracy: 0.9815\n",
      "Epoch 418/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.0587 - accuracy: 0.9775\n",
      "Epoch 419/2000\n",
      "75000/75000 [==============================] - 2s 27us/sample - loss: 0.0619 - accuracy: 0.9764\n",
      "Epoch 420/2000\n",
      "75000/75000 [==============================] - 2s 27us/sample - loss: 0.0521 - accuracy: 0.9801\n",
      "Epoch 421/2000\n",
      "75000/75000 [==============================] - 2s 26us/sample - loss: 0.0486 - accuracy: 0.9821\n",
      "Epoch 422/2000\n",
      "75000/75000 [==============================] - 2s 26us/sample - loss: 0.0520 - accuracy: 0.9801\n",
      "Epoch 423/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.0542 - accuracy: 0.9798\n",
      "Epoch 424/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.0579 - accuracy: 0.9779\n",
      "Epoch 425/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.0603 - accuracy: 0.9776\n",
      "Epoch 426/2000\n",
      "75000/75000 [==============================] - 2s 26us/sample - loss: 0.0512 - accuracy: 0.9813\n",
      "Epoch 427/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.0493 - accuracy: 0.9817\n",
      "Epoch 428/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.0553 - accuracy: 0.9795\n",
      "Epoch 429/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.0535 - accuracy: 0.9797\n",
      "Epoch 430/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.0553 - accuracy: 0.9790\n",
      "Epoch 431/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.0579 - accuracy: 0.9785\n",
      "Epoch 432/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.0528 - accuracy: 0.9806\n",
      "Epoch 433/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.0517 - accuracy: 0.9807\n",
      "Epoch 434/2000\n",
      "75000/75000 [==============================] - 2s 26us/sample - loss: 0.0483 - accuracy: 0.9816\n",
      "Epoch 435/2000\n",
      "75000/75000 [==============================] - 2s 26us/sample - loss: 0.0500 - accuracy: 0.9811\n",
      "Epoch 436/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.0497 - accuracy: 0.9815\n",
      "Epoch 437/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.0541 - accuracy: 0.9794\n",
      "Epoch 438/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.0561 - accuracy: 0.9783\n",
      "Epoch 439/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.0723 - accuracy: 0.9726\n",
      "Epoch 440/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.0587 - accuracy: 0.9780\n",
      "Epoch 441/2000\n",
      "75000/75000 [==============================] - 2s 26us/sample - loss: 0.0518 - accuracy: 0.9805\n",
      "Epoch 442/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.0531 - accuracy: 0.9799\n",
      "Epoch 443/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.0488 - accuracy: 0.9818\n",
      "Epoch 444/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.0532 - accuracy: 0.9797\n",
      "Epoch 445/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.0522 - accuracy: 0.9797\n",
      "Epoch 446/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.0541 - accuracy: 0.9794\n",
      "Epoch 447/2000\n",
      "75000/75000 [==============================] - 2s 25us/sample - loss: 0.0523 - accuracy: 0.9805\n",
      "Epoch 448/2000\n",
      "12000/75000 [===>..........................] - ETA: 1s - loss: 0.0480 - accuracy: 0.9816"
     ]
    }
   ],
   "source": [
    "model.fit(x=X_pad.values, y=df_train.sentiment.values, batch_size=6000, epochs=2000, callbacks=[tensorboard_callback, \n",
    "                                                                                                early_stop, checkpoint_callback])\n",
    "#Write X_pad.values if it's trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f63cc39fad0>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_weights(path_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75000/75000 [==============================] - 58s 773us/sample - loss: 7.8612e-04 - accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.0007861169725145738, 0.99998665]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_pad.values, df_train.sentiment.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-01T18:07:47.170335Z",
     "start_time": "2019-12-01T18:06:45.121236Z"
    }
   },
   "outputs": [],
   "source": [
    "#Submit the results to kaggle\n",
    "X_submit = model.predict_classes(X_pad_test.values)\n",
    "submission = pd.merge(df_test.id, pd.DataFrame(X_submit, columns=['sentiment']), left_index=True, right_index=True)\n",
    "submission.to_csv('submission_w2v', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Glove2vec Pre trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-04T05:37:40.830965Z",
     "start_time": "2019-12-04T05:37:38.101384Z"
    }
   },
   "outputs": [],
   "source": [
    "model_glove = Sequential()\n",
    "model_glove.add(Embedding(input_dim = len(fourgram_model.wv.vectors) + 1,\n",
    "                   output_dim = EMBEDDING_DIM,\n",
    "                   input_length = input_length,\n",
    "                   weights = [embedding_matrix],\n",
    "                   trainable = False))\n",
    "# model_glove.add(Dropout(0.2))\n",
    "model_glove.add(Conv1D(64, 3, activation='relu', padding='same'))\n",
    "model_glove.add(MaxPool1D(pool_size=4))\n",
    "model_glove.add(Bidirectional(LSTM(128, recurrent_dropout=0.1)))\n",
    "model_glove.add(Dropout(0.25))\n",
    "model_glove.add(Dense(64))\n",
    "model_glove.add(Dropout(0.3))\n",
    "model_glove.add(Dense(1, activation='sigmoid'))\n",
    "model_glove.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model_glove.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-04T06:07:01.175478Z",
     "start_time": "2019-12-04T05:37:44.889131Z"
    }
   },
   "outputs": [],
   "source": [
    "model_glove.fit(x=X_pad.values, y=df_train.sentiment.values, batch_size=100, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-04T06:19:55.163894Z",
     "start_time": "2019-12-04T06:19:54.630978Z"
    }
   },
   "outputs": [],
   "source": [
    "model_glove.save('/home/alberto/Escritorio/pycharm/NLP/ignore_files/sentyment_model_G2V_trained.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-04T06:19:58.820007Z",
     "start_time": "2019-12-04T06:19:58.723322Z"
    }
   },
   "outputs": [],
   "source": [
    "model_glove.load_weights('/home/alberto/Escritorio/pycharm/NLP/ignore_files/sentyment_model_G2V_trained.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-04T06:21:04.952484Z",
     "start_time": "2019-12-04T06:20:01.884262Z"
    }
   },
   "outputs": [],
   "source": [
    "#Submit the results to kaggle\n",
    "X_submit = model_glove.predict_classes(X_pad_test.values)\n",
    "submission = pd.merge(df_test.id, pd.DataFrame(X_submit, columns=['sentiment']), left_index=True, right_index=True)\n",
    "submission.to_csv('submission_G2V', index=False)\n",
    "#Kaggle give us a 98% of accuracy wich is great"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "214.162px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "465.412px",
    "left": "1745.91px",
    "right": "20px",
    "top": "121.94px",
    "width": "471.293px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
